\section{Linear Equations}
\begin{outline}
    Let $T: \R ^ n \to \R ^ m$, $T(x) = Ax$, $A$ is an $m\times n$ matrix, and
    $Ax = b$ is a linear system.
    Logically equivalent:
    \begin{compactitem}
    \item $T$ is one-to-one (at most one solution for all vectors $b$).
    \item None of the columns of $A$ is a linear combination of the others.
    \item $Ax = 0$ has only the trivial solution.
    \item The columns of $A$ are linearly independent.
    \end{compactitem}

    \columnbreak

    Logically equivalent:
    \begin{compactitem}
    \item $T$ is onto (at least one solution for all vectors $b$).
    \item The columns of $A$ span $\R ^ m$.
    \item For all $b$, $b$ is a linear combination of the columns of $A$.
    \item For all $b$, $Ax = b$ has a solution.
    \item $A$ has a pivot in every row.
    \end{compactitem}
\end{outline}

\begin{card}
    \subsection{Systems of Linear Equations}

    \begin{compactdesc}
    \item[linear equation] equation that can be written as
        $a_1x_1 + \dots + a_nx_n = b$.
        Numbers $a_i$ are \textbf{coefficients}.
    \item[linear system] many linear equations.
    \item[solution] A number making each equation a true
        statement on substitution of the $x_i$ variables.
    \item[solution set] all possible solutions of a linear system
    \item[equivalence] of linear systems: same solution set? equal linear systems.
    \item[consistent] has one or infinitely many solutions. \textbf{Inconsistent} has none.
    \item[augmented matrix] matrix representation for linear system.
        Rows are equations, columns variables.
        \begin{align*}
        x_1 - 2x_2 + x_3   &= 0 \\
              2x_2 - 8x_3  &= 8 \\
        -4x_1 + 5x_2 + 9x_3 &= -9
        \end{align*}
        \[
        \left[
        \begin{array}{ccc|c}
            1 & -2 & 1 & 0  \\
            0 & 2  & -8 & 8 \\
            -4 & 5 & 9 & -9
        \end{array}
        \right]
        \]
    \item[$m \times n$ matrix] has m rows, n columns
    \item[solving linear system] replace system with an equivalent, but
        easier to solve.
    \item[elementary row operations] all reversible.
        \begin{compactenum}
        \item replace one row by the sum of itself and a multiple of
            another row
        \item interchange two rows
        \item multiply row by non-zero constant
        \end{compactenum}
    \item[row equivalence] if two matrices can be transformed into
        each other through row ops.
    \item[existence and uniqueness questions] fundamental question. Does
        a system have a solution, is it the only one?
    \end{compactdesc}
\end{card}



\begin{card}
    \subsection{Row Reduction and Echelon Forms}

    \begin{compactdesc}
    \item[leading entry] leftmost nonzero entry of a nonzero row
    \item[echelon form] (rectangular matrix)
        \begin{compactenum}
        \item nonzero rows above all-zero rows
        \item leading entry is to the right of leading entry above
        \item entries in a column below leading entry are zero
        \end{compactenum}
    \item[reduced row echelon form] AKA reduced echelon form.
        Must be in echelon form and:
        \begin{compactenum}
        \item leading entry in each nonzero row is 1
        \item leading 1 is the only nonzero in column
        \end{compactenum}
    \item[row reduction] is possible for any nonzero matrix

    \item[pivot positions] locations of the leading 1s in the RREF of a matrix.
        \textbf{Pivot columns} are the columns with such 1s.
    \item[observations] one for each row
    \item[variables] one for each column, excluding the right-most of
        the augmented matrix
    \item[basic variables] correspond to pivot columns
    \item[free variables] are not pivot columns. May take any value.
    \item[general solution] explicit description of all solutions, such
        as
        \begin{align*}
        S = \begin{cases}
        x_1 &= 1 + 5x_3 \\
        x_2 &= 4 \\
        x_3 &\text{ is free}
        \end{cases}
        \end{align*}
    \item[row reduction solving strategy]
        \begin{compactenum}
        \item Write augmented matrix
        \item Row reduction algorithm to obtain echelon form
        \item If consistent, continue. Otherwise no solution.
        \item Continue row reduction until RREF
        \item Write system of equations
        \item Rewrite nonzero equations so basic variables are expressed in
            terms of free variables.
        \end{compactenum}
    \end{compactdesc}
\end{card}
\begin{card}
    \begin{theorem}[Uniqueness of RREF]
        Each matrix is row equivalent to one and only one reduced
        echelon matrix.
    \end{theorem}

    \begin{theorem}[Existence and Uniqueness]
    A linear system is consistent iff the rightmost column of the
    augmented matrix is not a pivot column. Iff the aug. matrix
    has no row of form
    $[ 0, \dots 0, b ]; b \neq 0$.

    If the linear system is consistent, then it has a unique solution
    when there are no free variables and infinitely many solutions if
    there are.
    \end{theorem}

    \subsubsection{Row reduction algorithm}
    \begin{compactenum}
    \item Begin with the leftmost nonzero column, now a pivot column.
        Pivot position is at the top.
    \item Select nonzero as a pivot. Interchange rows as needed.
    \item Row replacement operations to create zeros below the pivot.
    \item Ignore pivot position's row and all above, pick a new row and repeat 1-3 until no
        more nonzero rows to modify.
    \item Make all pivots 1 by scaling operations.
    \end{compactenum}
\end{card}



\begin{card}
    \subsection{Vector Equations}

    \begin{compactdesc}
    \item[list of numbers] is an intuitive definition of a vector
        (until chapter 4). Example: $(1,2,3)$.
    \item[column vector] ordered arrays represented by $n\times1$ matrix.
    \item[scalar multiplication] multiply each entry in the vector by the scalar
    \item[vector sum] add corresponding entries
    \item[zero vector] all entries are zero
    \item[geometric interpretation] a point in n-dimensional space. Can also
        be an arrow from the origin to that point.
    \item[parallelogram addition rule] \textbf{u + v} corresponds to the
        fourth vertex of the parallelogram with vertices \textbf{u, v, 0}
    \item[Linear combination] The vector $\bm{y}$ is a linear combo of the
        vectors $\bm{v_i}$ given the scalars (or \textbf{weights}) $c_i$ if:
        \[
            \bm{y} = c_1 \bm{v_1} + \dots + c_p \bm{v_p}
        \]
    \item[$\bm{y}$ can be generated] by a linear combination $V = \bm{y}$ only
        if the augmented matrix $[V \text{ } \bm{y}]$ has a solution.
    \item[Span] of vectors $\text{Span}\{ \bm{v_1} \dots \bm{v_p} \}$ is
        called the subset of $\R^n$ spanned or generated by the
        vectors. All vectors that can be written with scalars $c_1 \dots c_p$
        as:
        \[
            c_1 v_1 + \dots + c_p v_p
        \]
    \item[Is a vector $b$ in a span] is tantamount to asking: does the
        vector equation $x_1 v_1 + \dots + x_p v_p = b$ have a solution?
    \item[Always in a span] the multiples of $v_i$, and the zero vector
    \item[Span\{v\}] is a \textbf{line} with all scalar multiples of \textbf{v}
        or the \textbf{origin} if $v = 0$.
    \item[Span\{u, v\}] is a \textbf{plane} containing \textbf{u, v, 0} when $u$ and
        $v$ are not scalar multiples of each other and when $u \neq 0$ and
        $v \neq 0$.
    \end{compactdesc}
    \textbf{Algebraic properties} of vectors in $\R^n$ space, let
        c, d be scalars and u, v be vectors in:
        \begin{compactenum}
        \item u + v = v + u
        \item (u + v) + w = u + (v + w)
        \item u + 0 = 0 + u = u
        \item u + (-u) = -u + u = 0
        \item -u = (-1)u
        \item c(u + v) = cu + cv
        \item (c + d)u = cu + du
        \item c(du) = (cd)u
        \item 1u = u
        \end{compactenum}

\end{card}



\begin{card}
    \subsection{Matrix Equation Ax = b}

    \begin{compactdesc}
    \item[fundamental idea] linear combination of vectors is the product of a
        matrix and a vector. Can rephrase concepts of section 1.3.
    \item[matrix $\times$ vector] linear combination of the columns using the
        corresponding entries of the vector as weights. Only defined if
        columns of A = entries in \textbf{x}.
        \[
            A\bm{x} = x_1 \bm{a_1} + \dots x_n \bm{a_n}
        \]
    \item[matrix equation] has form A\textbf{x = b} where A is a matrix, x,b
        are matrices.
    \item[Existence of solutions] $A\bm{x = b}$ has a solution iff \textbf{b}
        is a linear combination of the columns of A.
    \item[Row-vector rule] for matrix-vector product A\textbf{x}. If it is
        defined, then the $i$th entry in the result is the sum of the
        products of corresponding entries from row $i$ of A and
        from vector \textbf{x}.
        \[
            A\bm{x} = \left[ \begin{array}{c}
            \sum a_{1i} x_1
            \\
            \sum a_{2i} x_2
            \\
            \cdots
            \\
            \sum a_{ni} x_n
            \end{array} \right]
        \]
    \item[Identity matrix] has 1s on diagonal and 0s everywhere else.
        I\textbf{x = x} for every $\bm{x} \in \R^n$
    \item[Proof of theorem 1.4] statements $1,2,3$ have been shown to be true;
        now assume statement 4 is false. Then the augmented matrix U would
        be inconsistent and A\textbf{x = b} would have no solution.
        If statement 4 is true, then the system is consistent and has
        at least one solution.

    \end{compactdesc}

\end{card}
\begin{card}
    \begin{theorem}[Notation]
        If A is an $m \times n$ matrix and $b \in \R^n$ then the
        matrix equation, vector equation and augmented matrix share the same
        solution set:
        \begin{align*}
        A\bm{x = b}
        \\
        x_1 \bm{a_1} + \cdots + x_n \bm{a_n = b}
        \\
        [ \begin{array}{ccc|c} \bm{a_1} & \cdots & \bm{a_n}
                             & \bm{b} \end{array} ]
        \end{align*}
    \end{theorem}

    \begin{theorem}[Span, linear combo., pivots]
    Let A be an $m \times n$ matrix. The following statements are logically
    equivalent:
    \begin{compactenum}
    \item For each $\bm{b} \in \R^n$, the equation
        $A\bm{x = b}$ has a solution.
    \item Each $\bm{b} \in \R^n$ is a linear combination of the
        columns in A.
    \item The columns of A span $\R^n$.
    \item A has a pivot position in every row. That is the RREF of A (not the
    augmented matrix) has a pivot in every row.
    \end{compactenum}
    \end{theorem}

    \begin{theorem}[Algebraic properties of matrix $\times$ vector]
        Let $A$ be an $m \times n$ matrix, $u, v$ be $n$ length vectors,
        and $c$ is a scalar.
        \begin{compactenum}
        \item $A(u + v) = Au + Av$
        \item $A(cu) = c(Au)$
        \item $uA$ is undefined
        \end{compactenum}
    \end{theorem}

\end{card}



\begin{card}
    \subsection{Solution Sets of Linear Systems}

    \begin{compactdesc}
    \item[homogeneous] systems have $\bm{b = 0}$.
    \item[trivial solution] is always $0$.
    \item[nontrivial solution] the system has a free variable.
    \item[Span] of vectors $\{v_i\}$ can be used to represent a solution set
        of $Ax = 0$ using the right $v_i$ vectors. The set Span$\{0\}$
        represents systems with only the trivial solution.
    \item[1 free variable] or more iff there's a nontrivial solution.
    \item[parametric vector equation] solution set described using free variables
        as parameters. Solution is a linear combination of the vectors.
        \[
            \bm{x} = s\bm{u} + t\bm{v} \quad (s,t \in \R)
        \]
    \item[parametric vector form] parametric vector equation with the vectors
        $\bm{u} \cdots$ written explicitly.
    \item[solution of non-homogeneous]
        Any of the homo. solutions: $v_h$, then nonhomo. solutions: $v_h + p$
        where $p$ is any solution of the system.
        A non-homogeneous system has the same solution set, just translated
        by vector $p$.
        This only applies if nonhomo. system has at least one solution.
    \item[vector translation] adding a vector, think of moving it around.
    \item[writing a solution set] of a consistent system in parametric vector
        form
    \end{compactdesc}

    \begin{theorem}[solution set of non/homogeneous]
        Suppose the equation $A\bm{x} = \bm{b}$ is consistent and let $\bm{p}$
        be a solution. Solution set of is the set of all
        vectors of form $\bm{w} = \bm{p} + \bm{v_h}$, where $\bm{v_h}$ is
        any solution of $A\bm{x} = \bm{0}$.
        This only applies if the system has nontrivial solutions.
    \end{theorem}
\end{card}



\begin{card}[1]
    \subsection{Applications}

    \begin{compactdesc}
    \item[homogeneous system in economics] equilibrium between input and
        output. Quick way to turn input-output table into matrix: identity
        matrix - IO table.
    \item[Leontief exchange model] simpler version of the production model.
        No demand vector. Assumed that everything produced is consumed by
        the ``productive'' sectors of the economy.
    \item[Example]
    \item[balancing chemical equations] yup
    \item[network flow] write equations: sum of flow in = sum of flow out
    \end{compactdesc}
\end{card}



\begin{card}
    \subsection{Linear Independence}

    \begin{compactdesc}
    \item[independence] a set of $p$ vectors in $\R^n$ space is linear
        independence if $x_1 \bm{v_1} \dots x_p \bm{v_p} = \bm{0}$ or $Ax = 0$
        has only the trivial solution.
    \item[dependent] if non-zero weights $c_i$ such that
        $c_1 \bm{v_1} \dots c_p \bm{v_p} = \bm{0}$.
    \item[linear combination] a linear dependence represents a nontrivial
        solution: $\bm{v_p} = (c_1 \bm{v_1} \dots c_{p-1} \bm{v_{p-1}}) / c_p$
    \item[linear dependence relation] called so only when all weights $c_i$
        are non-zero.
    \item[linear independence of columns] in a matrix happens only when
        $A\bm{x = 0}$ has only the trivial solution.
    \item[one vector] is linear independent iff it is not the zero vector.
    \item[two vectors] are linearly dependent when they are multiples of each
        other.
    \item[proof of theorem 7] Assume a vector is a linear combination of the
        others. Subtract it to produce a homogeneous equation with at least one
        nonzero coefficient $-1$. Thus the vectors are linearly dependent.

        Now assume the set is linearly independent. Then the homogeneous equation
        can be rewritten by subtracting one of the nonzero vectors and
        dividing its coefficient to get a linear combination. QED
    \item[proof of theorem 8] if there are more columns than rows, then there
        must be a free variable. This means $Ax = 0$ has a nontrivial solution.
    \item[proof of theorem 9] consider the equation with $v_1 = 0$,
        $1v_1 + 0v_2 \dots + 0v_p = 0$. It must be linearly dependent.
    \end{compactdesc}

    \begin{theorem}[Characterization of Linearly Dep. Sets]
        A linearly dependent set has \textbf{at least one} vector that is a
        linear combination of the others.
    \end{theorem}

    \begin{theorem}[More vectors than vector entries]
        Must be dependent.
    \end{theorem}

    \begin{theorem}[Contains Zero vector]
        Must be dependent.
    \end{theorem}
\end{card}



\begin{card}
    \subsection{Linear Transformations}

    \begin{compactdesc}
    \item[transformation, function, mapping] a rule that assigns one
        vector from $\R^n$ to another in $\R^m$, where $n$ may equal $m$.
    \item[domain] set of all inputs
    \item[codomain] subset of outputs
    \item[range] all possible outputs,
        a.k.a. \textbf{image}.
    \item[matrix transformation] transforms a vector from $\R^n$ to one from
        $\R^m$ through matrix multiplication $x \mapsto Ax$.
    \item[linear transformation] a transformation $T$ is linear (and can be
        represented by matrix multiplication) iff $T(u + v) = T(u) + T(v)$
        and $T(cu) = cT(u)$ for vectors $u, v$ and the scalar $c$.
    \item[$T(0) = 0$ follows] because $T(0) = T(0u) = 0T(u) = 0$.
    \end{compactdesc}
\end{card}



\begin{card}
    \subsection{Matrix of a Linear Transformations}

    \begin{compactdesc}
    \item[standard matrix for a linear trans.] all linear transformations
        can be defined using a standard matrix $A$ like this: $T(x) = Ax$.
        If $T: \R^n \to \R^m$ then $A$ is $m \times n$.
    \item[solving for standard matrix] $e_i$ is the ith column of the identity
    matrix. $A = \begin{bmatrix}T(e_1) & T(e_2) & \cdots & T(e_n) \end{bmatrix}$
    \item[geometric linear trans.] think of performing the $\R^2$ transformation
        on a unit square because $T$ is solely defined by its actions on the
        identity matrix $I$.
    \item[onto mappings] $T$ is maps onto $\R^m$ if every $b$ in $\R^m$ is
        the image of \textbf{at least one} $x \in \R^n$
    \item[one-to-one mappings] $T$ is one-to-one if every $b$ in $\R^m$ is
        the image of \textbf{at most one} $x \in \R^n$
    \item[proof of theorem 11]
        Assume $T$ is one-to-one. Then $T(0) = 0$ counts as one solution
        and only the trivial solution. Now assume $T$ is not one-to-one. Then
        there exist at least two distinct vectors $T(u) = b = T(v)$.
        Because they are distinct, subtracting them gives another solution to $T(x) = 0$,
        $T(u - v) = b - b = 0$.
    \item[proof of theorem 12]
        (a) The columns of $A$ span $\R^m$ iff $Ax = b$ is consistent for
        every $b$. In other words, if $T(x) = b$ has at least one
        solution. Thus $T$ is onto.

        (b) So $T$ is one-to-one iff $Ax = 0$ has only the trivial solution.
        This happens iff the columns of $A$ are linearly independent as
        was noted previously.
    \end{compactdesc}


    \begin{theorem}[Linear trans., unique matrix]
        If $T: \R^n \to \R^m$ then there exists a unique $m\times n$
        matrix $A$ such that $T(x) = Ax$. This matrix is defined as
        $A = \begin{bmatrix}T(e_1) & T(e_2) & \cdots & T(e_n) \end{bmatrix}$
    \end{theorem}

    \begin{theorem}[One-to-one, trivial]
        $T$ is one-to-one iff $T(x) = 0$ has only the trivial solution.
    \end{theorem}

    \begin{theorem}[Linear trans. and standard matrix]
        $T: \R^n \to \R^m$ and $A$ is an $m \times n$ matrix.
        (a) $T$ maps $\R^n$ onto $\R^m$ iff the columns of $A$ span $\R^m$.

        (b) $T$ is one-to-one iff the columns of $A$ are linearly independent.
    \end{theorem}

    Some super fun linear transformations
    \begin{compactdesc}
    \item[rotation]
    \item[reflection]
    across $x_1$ axis would be $\begin{bmatrix}1 & 0 \\ 0 & -1\end{bmatrix}$
    \item[contraction/expansion] scale by a factor of $k$:
        $\begin{bmatrix} k & 0 \\ 0 & k \end{bmatrix}$
    \item[shears]
    stretches the ``top'' more than the ``bottom,'' or vice-verse
    (slant-like)
        $\begin{bmatrix} 1 & k \\ 0 & 1 \end{bmatrix}$
    \item[projections]
        a lot like $\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$
    \end{compactdesc}
\end{card}



\begin{card}
    \subsection{Examples of Linear Models}

    \begin{compactdesc}
    \item[Kirchhoff's law] see below
    \item[DC networks] sum of all voltages in a loop = sum of all (resistors
        * adjacent current). Flows from positive to negative, otherwise
        negate the voltage.
    \item[difference equations] $x_{k+1} = Ax_k$ a recurrence relation.
    \end{compactdesc}
\end{card}



