\section{Linear Equations}
\begin{outline}
    Let $T: \R ^ n \to \R ^ m$, $T(x) = Ax$, $A$ is an $m\times n$ matrix, and
    $Ax = b$ is a linear system.
    Logically equivalent:
    \begin{compactitem}
    \item $T$ is one-to-one (at most one solution for all vectors $b$).
    \item None of the columns of $A$ is a linear combination of the others.
    \item $Ax = 0$ has only the trivial solution.
    \item The columns of $A$ are linearly independent.
    \end{compactitem}

    \columnbreak

    Logically equivalent:
    \begin{compactitem}
    \item $T$ is onto (at least one solution for all vectors $b$).
    \item The columns of $A$ span $\R ^ m$.
    \item For all $b$, $b$ is a linear combination of the columns of $A$.
    \item For all $b$, $Ax = b$ has a solution.
    \item $A$ has a pivot in every row.
    \end{compactitem}
\end{outline}

\begin{card}
    \subsection{Systems of Linear Equations}

    \begin{compactdesc}
    \item[linear equation] equation that can be written as
        $a_1x_1 + \dots + a_nx_n = b$.
        Numbers $a_i$ are \textbf{coefficients}.
    \item[linear system] many linear equations.
    \item[solution] A number making each equation a true
        statement on substitution of the $x_i$ variables.
    \item[solution set] all possible solutions of a linear system
    \item[equivalence] of linear systems: same solution set? equal linear systems.
    \item[consistent] has one or infinitely many solutions. \textbf{Inconsistent} has none.
    \item[augmented matrix] matrix representation for linear system.
        Rows are equations, columns variables.
        \begin{align*}
        x_1 - 2x_2 + x_3   &= 0 \\
              2x_2 - 8x_3  &= 8 \\
        -4x_1 + 5x_2 + 9x_3 &= -9
        \end{align*}
        \[
        \left[
        \begin{array}{ccc|c}
            1 & -2 & 1 & 0  \\
            0 & 2  & -8 & 8 \\
            -4 & 5 & 9 & -9
        \end{array}
        \right]
        \]
    \item[$m \times n$ matrix] has m rows, n columns
    \item[solving linear system] replace system with an equivalent, but
        easier to solve.
    \item[elementary row operations] all reversible.
        \begin{compactenum}
        \item replace one row by the sum of itself and a multiple of
            another row
        \item interchange two rows
        \item multiply row by non-zero constant
        \end{compactenum}
    \item[row equivalence] if two matrices can be transformed into
        each other through row ops.
    \item[existence and uniqueness questions] fundamental question. Does
        a system have a solution, is it the only one?
    \end{compactdesc}
\end{card}



\begin{card}
    \subsection{Row Reduction and Echelon Forms}

    \begin{compactdesc}
    \item[leading entry] leftmost nonzero entry of a nonzero row
    \item[echelon form] (rectangular matrix)
        \begin{compactenum}
        \item nonzero rows above all-zero rows
        \item leading entry is to the right of leading entry above
        \item entries in a column below leading entry are zero
        \end{compactenum}
    \item[reduced row echelon form] AKA reduced echelon form.
        Must be in echelon form and:
        \begin{compactenum}
        \item leading entry in each nonzero row is 1
        \item leading 1 is the only nonzero in column
        \end{compactenum}
    \item[row reduction] is possible for any nonzero matrix

    \item[pivot positions] locations of the leading 1s in the RREF of a matrix.
        \textbf{Pivot columns} are the columns with such 1s.
    \item[observations] one for each row
    \item[variables] one for each column, excluding the right-most of
        the augmented matrix
    \item[basic variables] correspond to pivot columns
    \item[free variables] are not pivot columns. May take any value.
    \item[general solution] explicit description of all solutions, such
        as
        \begin{align*}
        S = \begin{cases}
        x_1 &= 1 + 5x_3 \\
        x_2 &= 4 \\
        x_3 &\text{ is free}
        \end{cases}
        \end{align*}
    \item[row reduction solving strategy]
        \begin{compactenum}
        \item Write augmented matrix
        \item Row reduction algorithm to obtain echelon form
        \item If consistent, continue. Otherwise no solution.
        \item Continue row reduction until RREF
        \item Write system of equations
        \item Rewrite nonzero equations so basic variables are expressed in
            terms of free variables.
        \end{compactenum}
    \end{compactdesc}
\end{card}
\begin{card}
    \begin{theorem}[Uniqueness of RREF]
        Each matrix is row equivalent to one and only one reduced
        echelon matrix.
    \end{theorem}

    \begin{theorem}[Existence and Uniqueness]
    A linear system is consistent iff the rightmost column of the
    augmented matrix is not a pivot column. Iff the aug. matrix
    has no row of form
    $[ 0, \dots 0, b ]; b \neq 0$.

    If the linear system is consistent, then it has a unique solution
    when there are no free variables and infinitely many solutions if
    there are.
    \end{theorem}

    \subsubsection{Row reduction algorithm}
    \begin{compactenum}
    \item Begin with the leftmost nonzero column, now a pivot column.
        Pivot position is at the top.
    \item Select nonzero as a pivot. Interchange rows as needed.
    \item Row replacement operations to create zeros below the pivot.
    \item Ignore pivot position's row and all above, pick a new row and repeat 1-3 until no
        more nonzero rows to modify.
    \item Make all pivots 1 by scaling operations.
    \end{compactenum}
\end{card}



\begin{card}
    \subsection{Vector Equations}

    \begin{compactdesc}
    \item[list of numbers] is an intuitive definition of a vector
        (until chapter 4). Example: $(1,2,3)$.
    \item[column vector] ordered arrays represented by $n\times1$ matrix.
    \item[scalar multiplication] multiply each entry in the vector by the scalar
    \item[vector sum] add corresponding entries
    \item[zero vector] all entries are zero
    \item[geometric interpretation] a point in n-dimensional space. Can also
        be an arrow from the origin to that point.
    \item[parallelogram addition rule] \textbf{u + v} corresponds to the
        fourth vertex of the parallelogram with vertices \textbf{u, v, 0}
    \item[Linear combination] The vector $\bm{y}$ is a linear combo of the
        vectors $\bm{v_i}$ given the scalars (or \textbf{weights}) $c_i$ if:
        \[
            \bm{y} = c_1 \bm{v_1} + \dots + c_p \bm{v_p}
        \]
    \item[$\bm{y}$ can be generated] by a linear combination $V = \bm{y}$ only
        if the augmented matrix $[V \text{ } \bm{y}]$ has a solution.
    \item[Span] of vectors $\text{Span}\{ \bm{v_1} \dots \bm{v_p} \}$ is
        called the subset of $\R^n$ spanned or generated by the
        vectors. All vectors that can be written with scalars $c_1 \dots c_p$
        as:
        \[
            c_1 v_1 + \dots + c_p v_p
        \]
    \item[Is a vector $b$ in a span] is tantamount to asking: does the
        vector equation $x_1 v_1 + \dots + x_p v_p = b$ have a solution?
    \item[Always in a span] the multiples of $v_i$, and the zero vector
    \item[Span\{v\}] is a \textbf{line} with all scalar multiples of \textbf{v}
        or the \textbf{origin} if $v = 0$.
    \item[Span\{u, v\}] is a \textbf{plane} containing \textbf{u, v, 0} when $u$ and
        $v$ are not scalar multiples of each other and when $u \neq 0$ and
        $v \neq 0$.
    \end{compactdesc}
    \textbf{Algebraic properties} of vectors in $\R^n$ space, let
        c, d be scalars and u, v be vectors in:
        \begin{compactenum}
        \item u + v = v + u
        \item (u + v) + w = u + (v + w)
        \item u + 0 = 0 + u = u
        \item u + (-u) = -u + u = 0
        \item -u = (-1)u
        \item c(u + v) = cu + cv
        \item (c + d)u = cu + du
        \item c(du) = (cd)u
        \item 1u = u
        \end{compactenum}

\end{card}



\begin{card}
    \subsection{Matrix Equation Ax = b}

    \begin{compactdesc}
    \item[fundamental idea] linear combination of vectors is the product of a
        matrix and a vector. Can rephrase concepts of section 1.3.
    \item[matrix $\times$ vector] linear combination of the columns using the
        corresponding entries of the vector as weights. Only defined if
        columns of A = entries in \textbf{x}.
        \[
            A\bm{x} = x_1 \bm{a_1} + \dots x_n \bm{a_n}
        \]
    \item[matrix equation] has form A\textbf{x = b} where A is a matrix, x,b
        are matrices.
    \item[Existence of solutions] $A\bm{x = b}$ has a solution iff \textbf{b}
        is a linear combination of the columns of A.
    \item[Row-vector rule] for matrix-vector product A\textbf{x}. If it is
        defined, then the $i$th entry in the result is the sum of the
        products of corresponding entries from row $i$ of A and
        from vector \textbf{x}.
        \[
            A\bm{x} = \left[ \begin{array}{c}
            \sum a_{1i} x_1
            \\
            \sum a_{2i} x_2
            \\
            \cdots
            \\
            \sum a_{ni} x_n
            \end{array} \right]
        \]
    \item[Identity matrix] has 1s on diagonal and 0s everywhere else.
        I\textbf{x = x} for every $\bm{x} \in \R^n$
    \item[Proof of theorem 1.4] statements $1,2,3$ have been shown to be true;
        now assume statement 4 is false. Then the augmented matrix U would
        be inconsistent and A\textbf{x = b} would have no solution.
        If statement 4 is true, then the system is consistent and has
        at least one solution.

    \end{compactdesc}

\end{card}
\begin{card}
    \begin{theorem}[Notation]
        If A is an $m \times n$ matrix and $b \in \R^n$ then the
        matrix equation, vector equation and augmented matrix share the same
        solution set:
        \begin{align*}
        A\bm{x = b}
        \\
        x_1 \bm{a_1} + \cdots + x_n \bm{a_n = b}
        \\
        [ \begin{array}{ccc|c} \bm{a_1} & \cdots & \bm{a_n}
                             & \bm{b} \end{array} ]
        \end{align*}
    \end{theorem}

    \begin{theorem}[Span, linear combo., pivots]
    Let A be an $m \times n$ matrix. The following statements are logically
    equivalent:
    \begin{compactenum}
    \item For each $\bm{b} \in \R^n$, the equation
        $A\bm{x = b}$ has a solution.
    \item Each $\bm{b} \in \R^n$ is a linear combination of the
        columns in A.
    \item The columns of A span $\R^n$.
    \item A has a pivot position in every row. That is the RREF of A (not the
    augmented matrix) has a pivot in every row.
    \end{compactenum}
    \end{theorem}

    \begin{theorem}[Algebraic properties of matrix $\times$ vector]
        Let $A$ be an $m \times n$ matrix, $u, v$ be $n$ length vectors,
        and $c$ is a scalar.
        \begin{compactenum}
        \item $A(u + v) = Au + Av$
        \item $A(cu) = c(Au)$
        \item $uA$ is undefined
        \end{compactenum}
    \end{theorem}

\end{card}



\begin{card}
    \subsection{Solution Sets of Linear Systems}

    \begin{compactdesc}
    \item[homogeneous] systems have $\bm{b = 0}$.
    \item[trivial solution] is always $0$.
    \item[nontrivial solution] the system has a free variable.
    \item[Span] of vectors $\{v_i\}$ can be used to represent a solution set
        of $Ax = 0$ using the right $v_i$ vectors. The set Span$\{0\}$
        represents systems with only the trivial solution.
    \item[1 free variable] or more iff there's a nontrivial solution.
    \item[parametric vector equation] solution set described using free variables
        as parameters. Solution is a linear combination of the vectors.
        \[
            \bm{x} = s\bm{u} + t\bm{v} \quad (s,t \in \R)
        \]
    \item[parametric vector form] parametric vector equation with the vectors
        $\bm{u} \cdots$ written explicitly.
    \item[solution of non-homogeneous]
        Any of the homo. solutions: $v_h$, then nonhomo. solutions: $v_h + p$
        where $p$ is any solution of the system.
        A non-homogeneous system has the same solution set, just translated
        by vector $p$.
        This only applies if nonhomo. system has at least one solution.
    \item[vector translation] adding a vector, think of moving it around.
    \item[writing a solution set] of a consistent system in parametric vector
        form
    \end{compactdesc}

    \begin{theorem}[solution set of non/homogeneous]
        Suppose the equation $A\bm{x} = \bm{b}$ is consistent and let $\bm{p}$
        be a solution. Solution set of is the set of all
        vectors of form $\bm{w} = \bm{p} + \bm{v_h}$, where $\bm{v_h}$ is
        any solution of $A\bm{x} = \bm{0}$.
        This only applies if the system has nontrivial solutions.
    \end{theorem}
\end{card}



\begin{card}[1]
    \subsection{Applications}

    \begin{compactdesc}
    \item[homogeneous system in economics] equilibrium between input and
        output. Quick way to turn input-output table into matrix: identity
        matrix - IO table.
    \item[Leontief exchange model] simpler version of the production model.
        No demand vector. Assumed that everything produced is consumed by
        the ``productive'' sectors of the economy.
    \item[Example]
    \item[balancing chemical equations] yup
    \item[network flow] write equations: sum of flow in = sum of flow out
    \end{compactdesc}
\end{card}



\begin{card}
    \subsection{Linear Independence}

    \begin{compactdesc}
    \item[independence] a set of $p$ vectors in $\R^n$ space is linear
        independence if $x_1 \bm{v_1} \dots x_p \bm{v_p} = \bm{0}$ or $Ax = 0$
        has only the trivial solution.
    \item[dependent] if non-zero weights $c_i$ such that
        $c_1 \bm{v_1} \dots c_p \bm{v_p} = \bm{0}$.
    \item[linear combination] a linear dependence represents a nontrivial
        solution: $\bm{v_p} = (c_1 \bm{v_1} \dots c_{p-1} \bm{v_{p-1}}) / c_p$
    \item[linear dependence relation] called so only when all weights $c_i$
        are non-zero.
    \item[linear independence of columns] in a matrix happens only when
        $A\bm{x = 0}$ has only the trivial solution.
    \item[one vector] is linear independent iff it is not the zero vector.
    \item[two vectors] are linearly dependent when they are multiples of each
        other.
    \item[proof of theorem 7] Assume a vector is a linear combination of the
        others. Subtract it to produce a homogeneous equation with at least one
        nonzero coefficient $-1$. Thus the vectors are linearly dependent.

        Now assume the set is linearly independent. Then the homogeneous equation
        can be rewritten by subtracting one of the nonzero vectors and
        dividing its coefficient to get a linear combination. QED
    \item[proof of theorem 8] if there are more columns than rows, then there
        must be a free variable. This means $Ax = 0$ has a nontrivial solution.
    \item[proof of theorem 9] consider the equation with $v_1 = 0$,
        $1v_1 + 0v_2 \dots + 0v_p = 0$. It must be linearly dependent.
    \end{compactdesc}

    \begin{theorem}[Characterization of Linearly Dep. Sets]
        A linearly dependent set has \textbf{at least one} vector that is a
        linear combination of the others.
    \end{theorem}

    \begin{theorem}[More vectors than vector entries]
        Must be dependent.
    \end{theorem}

    \begin{theorem}[Contains Zero vector]
        Must be dependent.
    \end{theorem}
\end{card}



\begin{card}
    \subsection{Linear Transformations}

    \begin{compactdesc}
    \item[transformation, function, mapping] a rule that assigns one
        vector from $\R^n$ to another in $\R^m$, where $n$ may equal $m$.
    \item[domain] set of all inputs
    \item[codomain] subset of outputs
    \item[range] all possible outputs,
        a.k.a. \textbf{image}.
    \item[matrix transformation] transforms a vector from $\R^n$ to one from
        $\R^m$ through matrix multiplication $x \mapsto Ax$.
    \item[linear transformation] a transformation $T$ is linear (and can be
        represented by matrix multiplication) iff $T(u + v) = T(u) + T(v)$
        and $T(cu) = cT(u)$ for vectors $u, v$ and the scalar $c$.
    \item[$T(0) = 0$ follows] because $T(0) = T(0u) = 0T(u) = 0$.
    \end{compactdesc}
\end{card}



\begin{card}
    \subsection{Matrix of a Linear Transformations}

    \begin{compactdesc}
    \item[standard matrix for a linear trans.] all linear transformations
        can be defined using a standard matrix $A$ like this: $T(x) = Ax$.
        If $T: \R^n \to \R^m$ then $A$ is $m \times n$.
    \item[solving for standard matrix] $e_i$ is the ith column of the identity
    matrix. $A = \begin{bmatrix}T(e_1) & T(e_2) & \cdots & T(e_n) \end{bmatrix}$
    \item[geometric linear trans.] think of performing the $\R^2$ transformation
        on a unit square because $T$ is solely defined by its actions on the
        identity matrix $I$.
    \item[onto mappings] $T$ is maps onto $\R^m$ if every $b$ in $\R^m$ is
        the image of \textbf{at least one} $x \in \R^n$
    \item[one-to-one mappings] $T$ is one-to-one if every $b$ in $\R^m$ is
        the image of \textbf{at most one} $x \in \R^n$
    \item[proof of theorem 11]
        Assume $T$ is one-to-one. Then $T(0) = 0$ counts as one solution
        and only the trivial solution. Now assume $T$ is not one-to-one. Then
        there exist at least two distinct vectors $T(u) = b = T(v)$.
        Because they are distinct, subtracting them gives another solution to $T(x) = 0$,
        $T(u - v) = b - b = 0$.
    \item[proof of theorem 12]
        (a) The columns of $A$ span $\R^m$ iff $Ax = b$ is consistent for
        every $b$. In other words, if $T(x) = b$ has at least one
        solution. Thus $T$ is onto.

        (b) So $T$ is one-to-one iff $Ax = 0$ has only the trivial solution.
        This happens iff the columns of $A$ are linearly independent as
        was noted previously.
    \end{compactdesc}


    \begin{theorem}[Linear trans., unique matrix]
        If $T: \R^n \to \R^m$ then there exists a unique $m\times n$
        matrix $A$ such that $T(x) = Ax$. This matrix is defined as
        $A = \begin{bmatrix}T(e_1) & T(e_2) & \cdots & T(e_n) \end{bmatrix}$
    \end{theorem}

    \begin{theorem}[One-to-one, trivial]
        $T$ is one-to-one iff $T(x) = 0$ has only the trivial solution.
    \end{theorem}

    \begin{theorem}[Linear trans. and standard matrix]
        $T: \R^n \to \R^m$ and $A$ is an $m \times n$ matrix.
        (a) $T$ maps $\R^n$ onto $\R^m$ iff the columns of $A$ span $\R^m$.

        (b) $T$ is one-to-one iff the columns of $A$ are linearly independent.
    \end{theorem}

    Some super fun linear transformations
    \begin{compactdesc}
    \item[rotation]
    \item[reflection]
    across $x_1$ axis would be $\begin{bmatrix}1 & 0 \\ 0 & -1\end{bmatrix}$
    \item[contraction/expansion] scale by a factor of $k$:
        $\begin{bmatrix} k & 0 \\ 0 & k \end{bmatrix}$
    \item[shears]
    stretches the ``top'' more than the ``bottom,'' or vice-verse
    (slant-like)
        $\begin{bmatrix} 1 & k \\ 0 & 1 \end{bmatrix}$
    \item[projections]
        a lot like $\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$
    \end{compactdesc}
\end{card}



\begin{card}
    \subsection{Examples of Linear Models}

    \begin{compactdesc}
    \item[Kirchhoff's law] see below
    \item[DC networks] sum of all voltages in a loop = sum of all (resistors
        * adjacent current). Flows from positive to negative, otherwise
        negate the voltage.
    \item[difference equations] $x_{k+1} = Ax_k$ a recurrence relation.
    \end{compactdesc}
\end{card}



\section{Matrix Algebra}
\begin{outline}
\end{outline}

\begin{card}
    \subsection{Matrix Operations}

    \begin{compactdesc}
    \item[matrix] has $m$ rows and $n$ columns, denoted as $m \times n$.
        The element in the ith row and jth column is represented by $A_{ij}$.
    \item[zero matrix] all entries are zero
    \item[identity matrix] all entries on the diagonal are one and the rest are
        zero.
    \item[sane addition and scalar] multiplication rules for two matrices
        $A,B$ and two scalars $r,s$.
        \begin{compactenum}
        \item $A + B = B + A$
        \item $( A + B) + C = A + (B + C)$
        \item $A + 0 = A$
        \item $r(A + B) = rA + rB$
        \item $(r + s)A = rA + sA$
        \item $r(sA) = (rs)A$
        \end{compactenum}
    \item[proofs] by decomposing matrices into scalars $A_{ij}$ or column vectors
        $a_i$. Show entries are equal and matrices are same size.
    \item[multiplication goal] could have element-wise multiplication, but
        it is better to have $A(Bx) = (AB)x$, composable matrix-vector
        multiplication.
    \item[matrix multiplication] only works for two matrices, $A$ of size
    $m \times n$ and $B$ of size $n \times p$. The result has size $n \times p$.
    Defined as $AB = \begin{bmatrix}Ab_1 & Ab_2 & \cdots & Ab_p \end{bmatrix}$.
    \item[row-column rule] for computing $AB$. The $(i,j)$ entry in
        $AB$ is $(AB)_{ij} = \sum^n_{k=1} A_{ik}B_{kj}$ where $n$ is the
        shared size.
    \item[WARNINGS]
    \item[Not commutative] $AB \neq BA$.
    \item[No cancellation] laws $\lnot(AB = AC \implies B = C)$ unless $A$ is
        invertible.
    \item[Zero is not the only] matrix that can create zero
        $AB = 0 \not\implies A = 0 \lor B = 0$.
    \item[powers] $A^k = A \cdots A$, the matrix $A$ multiplied by itself
        $k$ times. $A^0 = I$ so that $A^0x = x$.
    \item[transpose] $A^T$ is the $n \times m$ transpose of the $m \times n$
        sized matrix A. The entries are defined: $A^T_{ij} = A_{ji}$.
    \end{compactdesc}
\end{card}
\begin{card}
    \begin{theorem}[Matrix Multiplication rules]
        Let $A$ be an $m \times n$ matrix and let $B, C$ have sizes for which
        these sums and products are defined.
        \begin{compactenum}
        \item associative: $(AB)C = A(BC)$
        \item left distributive: $A(B + C) = AB + AC$
        \item right distributive: $(B + C)A = BA + CA$
        \item $r(AB) = (rA)B = A(rB)$ for any scalar $r$
        \item identity elements: $I_mA = A = AI_n$.
        \end{compactenum}
    \end{theorem}
    \begin{theorem}[Transpose rules]
        Let $A, B$ be matrices whose sizes are appropriate for the following
        sums and products.
        \begin{compactenum}
        \item $(A^T)^T = A$
        \item $(A + B)^T = A^T + B^T$
        \item For any scalar $r, (rA)^T = rA^t$
        \item $(AB)^T = B^TA^T$
        \end{compactenum}
    \end{theorem}

\end{card}


\begin{card}
    \subsection{Inverse of a Matrix}

    \begin{compactdesc}
    \item[inverse] of a matrix $A^{-1}$ is a unique element which exists only
        for certain square matrices. Properties: $AA^{-1} = A^{-1}A = I$.
    \item[invertible] matrices have an inverse. Their determinant is not zero.
        Also called nonsingular.
    \item[noninvertible] matrices are called singular or degenerate.
    \item[inverse of $\R^2$] matrix is easy to calculate
    \item[proof of Th. \ref{th-inv-soln}]
        Take any $b \in \R^n$. The solution $x = A^{-1}b$ exists. Simply
        substitute to see. $x$ is the only solution. Let $u$ be another
        arbitrary vector. If $Au = b$, then $Iu = A^{-1}b$ and $u = A^{-1}b$.
    \item[Row reduction] is faster than finding the inverse of a matrix.
        Except, perhaps, in $2\times2$ matrices.
    \item[Elementary matrix] $E$, an identity matrix after a single elementary row
        operation.
    \item[Inverse of elementary] matrix is the same matrix that transforms
        $E$ back into $I$. Generalized into Th. \ref{th-gen-inverse}
    \item[Algorithm for] finding the inverse of $A$. Row reduce the matrix
        $\begin{bmatrix}A & I\end{bmatrix}$. If the result has form
        $\begin{bmatrix}I & A^{-1}\end{bmatrix}$, then $A$ has an inverse.
        Otherwise, there is none.
    \item[Alternative algorithm] for finding one or two columns of $A^{-1}$.
        The columns of $A^{-1}$ are the solutions of the system
        $Ax = e_1; Ax = e_2; \cdots Ax = e_n$. So, to find the pth column
        of the inverse, simply solve $Ax = e_p$.
    \end{compactdesc}
\end{card}
\begin{card}
    \begin{theorem}[Inverse of 2 by 2]
    Let  $A = \begin{bmatrix}a & b \\ c & d\end{bmatrix}$.
    If its determinant $ad - bc \neq 0$, then the inverse of $A$ is
    $A^{-1} = (1/(ad - bc))\begin{bmatrix}d & -b \\ -c & a\end{bmatrix}$.
    If the determinant is zero, then $A$ is not invertible.
    The \textbf{determinant} of this matrix is $\det A = ad - bc$.
    \end{theorem}

    \begin{theorem}[Solutions if invertible]\label{th-inv-soln}
        If $A$ is an invertible $n \times n$ matrix, then for each $b \in \R^n$,
        the equation $Ax = b$ has the unique solution $x = A^{-1}b$.
    \end{theorem}

    \begin{theorem}[Rules and the Transpose]
    Assume $A$ is invertible.
    Assume $B$ is invertible and the same size as $A$.
    \begin{compactenum}
    \item $A^{-1}$ is invertible.
    \item $(A^{-1})^{-1} = A$.
    \item $(AB)^{-1} = B^{-1}A^{-1}$.
    \item $(A^T)^{-1} = (A^{-1})^T = A^{-T}$.
    \item The product of $n \times n$ invertible matrices is invertible and
        it is the product of their inverses in the reverse order.
    \end{compactenum}
    \end{theorem}

    \begin{theorem}[Inverse of a matrix]\label{th-gen-inverse}
        An $n\times n$ matrix $A$ is invertible iff $A$ is row equivalent to
        $I_n$. The same sequence that transforms $A$ to $I_n$ also transforms
        $I_n$ to $A^{-1}$.
    \end{theorem}
\end{card}


\begin{card}
    \subsection{Characterizations of Invertible Matrices}

    \begin{theorem}[Invertible Matrix Theorem]
    $A$ is a square $n \times n$ matrix. These statements are logically
    equivalent:
    \begin{compactenum}
    \item $A$ is an invertible matrix.
    \item $A$ is row equivalent to $I_n$.
    \item $A$ has $n$ pivot columns.
    \item The equation $Ax = 0$ has only the trivial solution.
    \item The columns of $A$ form a linearly independent set.
    \item The linear transformation $x \mapsto Ax$ is one-to-one.
    \item The equation $Ax = b$ has at least one solution for each $b \in \R^n$.
    \item The columns of $A$ span $\R^n$.
    \item The linear transformation $x \mapsto Ax$ maps $\R^n$ onto $\R^n$.
    \item There is a pair of matrices $C,D$ such that $CA = I_n$ and $AD = I_n$.
    \item $A^T$ is an invertible matrix.
    \end{compactenum}
    \end{theorem}

    \begin{compactdesc}
    \item[IMT] separates square matrices into two disjoint classes.
    \item[Only square matrices] can be treated with the IMT
    \item[Fact] if $AB = I_n$ then $A, B$ are both invertible with
        $B = A^{-1}; A = B^{-1}$.
    \item[Singular, degenrate] matrices are not invertible.
    \end{compactdesc}


    \begin{theorem}[Invertible Transformation]
        Let $T : \R^n \to \R^n$ with standard matrix $A$. Then $T$ is
        invertible iff $A$ is an invertible matrix. If so, then $S(x) = A^{-1}x$
        satisfies $T(S(x)) = x$ and $S(T(x)) = x$.
    \end{theorem}
\end{card}


\begin{card}
    \subsection{Partitioned Matrices}
    Matrices can be split into chunks to alleviate the burden of
    multiplying large matrices.

    % \begin{compactdesc}
    % \item[boss]
    % \end{compactdesc}
% \end{card}
%
%
% \begin{card}
    \subsection{Matrix Factorization}
    Matrices can be factored into a pair of upper and lower triangular matrices
    $A = LU$.

    % \begin{compactdesc}
    % \item[boss]
    % \end{compactdesc}
\end{card}


\begin{card}
    \subsection{Leontief Input-Output Model}

    \begin{compactdesc}
    \item[Production vector] $x$ lists the outputs of each of $n$ sectors of
        an economy
    \item[Unit consumption vector] for each sector, lists the inputs from
        other sectors to produce one unit of output.
    \item[Consumption matrix] $C$, is a matrix composed of all the unit cons.
        vectors.
    \item[Final demand] $d$, lists the amount of goods demanded by the
        nonproductive part of economy.
    \item[Production equation] $x = Cx + d$ or $(I - C)x = d$ or if
        $I - C$ is invertible, $x = (I - C)^{-1}d$.
    \item[Modification] If the demand changes to $d + \Delta d$, then the
        economy must shift by $\Delta x = (I - C)^{-1}\Delta d$.
    \item[Formula for $(I - C)^{-1}$] is similar to the geometric series:
        $(I - C)^{-1} = \sum^\infty_{n=0} C^n$.
    \item[Approximation] If $C^m \to 0$ quickly enough, then this approximation
        applies: $(I - C)^{-1} \approx \sum^m_{n=0} C^n$.
    \item[Example] manufacting is $c_1$, big agri is $c_2$, and services is
        $c_3$. If it takes 50 units from other parts of manufacturing,
        20 units from agriculture, and 10 units from services to produce 100
        units of manufacturing then:
        \[
            c_1 = \begin{bmatrix} 0.5 \\ 0.2 \\ 0.1 \end{bmatrix}
            \quad
            C = \begin{bmatrix} 0.5 & 0.4 & 0.2 \\ 0.2 & 0.3 & 0.1 \\ 0.1 & 0.1 & 0.3 \end{bmatrix}
            \quad
            d = \begin{bmatrix} 50 \\ 30 \\ 20 \end{bmatrix}
        \]
        This system can be solved to get $x = \begin{bmatrix} 226 \\ 119 \\ 78 \end{bmatrix}$.
    \end{compactdesc}
\end{card}


\begin{card}
    \subsection{Applications to Computer Graphics}
    So cool!
    3D to 2D projection * rotation * translation * scale * crappy toyota vertices $\mapsto$ pretty toyota vertices

    % \begin{compactdesc}
    % \item[boss]
    % \end{compactdesc}
\end{card}


\section{Determinants}\label{ch-dets}
\begin{outline}
\end{outline}

\begin{card}
    \subsection{Introduction to Determinants}

    \begin{compactdesc}
    \item[Simple Determinants] a $2 \times 2$ matrix has determinant
        $ad - bc = a_{11}d_{22} - b_{12}c_{21}$.
        A $1 \times 1$ matrix has determinant $a_{11}$
    \item[Deletion] of a row $i$ and a column $j$ of a matrix
        is represented as $A_{ij}$
    \item[Cofactor] along the $i$th row and $j$th columnt is
        $C_{ij} = (-1)^{i + j} \det A_{ij}$
    \end{compactdesc}

    \begin{theorem}[Cofactor Expansion]
        Let $A$ be an $n\times n$ matrix. The determinant can be found in
        two similar ways:

        Down column $j$ is, we have $\det A = \sum_{k=1}^n a_{kj} C_{kj}$.

        Along row $i$ is, we have $\det A = \sum_{k=1}^n a_{ik} C_{ik}$.
    \end{theorem}

    \begin{theorem}[Triangular matrix]
    A triangular matrix has a determinant equal to the product of the
        diagonal elements: $\det A = \prod a_{ii}$.
    \end{theorem}
\end{card}


\begin{card}
    \subsection{Properties of Determinants}
    %pg 209/193

    \begin{theorem}[Row Operations and Det]\label{th-rowop-det}
    Let $A$ be a square matrix.

    \begin{compactenum}
    \item If a multiple of one row of $A$ is added to another row to
        produce matrix $A'$, then $\det A = \det A'$.
    \item If two rows are interchanged to produce matrix $A'$, then
        $-\det A = \det A'$.
    \item If one row is multiplied by $k$ to produce a matrix $A'$, then
        $k \det A = \det A'$.
    \end{compactenum}
    \end{theorem}

    \begin{compactdesc}
    \item[Common use of Th \ref{th-rowop-det}] factor out a multiple of
        a row to simplify finding the determinant.
    \item[Row op based formula] for calculating determinant of a matrix $A$
        row equivalent to a matrix $U$ in echelon form.
        Let $r$ be the number of row exchanges it takes to transform $A$.
        Efficient!
        $$\det A = \begin{cases}
            (-1)^r \cdot \prod \text{ pivots in U} & \text{if A is invertible}
         \\ 0 & \text{if A is not invertible}
        \end{cases}$$
    \end{compactdesc}

    \begin{theorem}[IMT and non-zero Det]
        A square matrix $A$ is invertible iff $\det A \neq 0$.
    \end{theorem}

    \begin{theorem}[Column operations and Det]
        For a square matrix $A$, $\det A^T = \det A$.
    \end{theorem}

    \begin{theorem}[Multiplication and Det]\label{th-det-prod}
        If $A$ and $B$ are square matrices, then $\det AB = \det A \det B$.
    \end{theorem}

    WARNING: no analogue for summation of matrices $\det (A + B)$.

    \begin{compactdesc}
    \item[Determinant is a Linear] function for a certain set of matrices.
        Useful in advanced courses. Assume a matrix has constant columns
        except for one. This varying column is the parameter $x$ to
        a function $T(x) = \det \begin{pmatrix} a_1 \cdots x \cdots a_n \end{pmatrix}$.
        It can be shown that $T(cx) = cT(x)$ and $T(u + v) = T(u) + T(v)$.
    % \item[Proof of Th \ref{th-rowop-det}]
        % TODO
    % \item[Proof of Th \ref{th-det-prod}]
        % TODO
    \end{compactdesc}
\end{card}


\begin{card}
    \subsection{Cramer's Rule, Volume, Linear Transformations}

    \begin{theorem}[Cramer's Rule]\label{th-cramer}
    Let $A$ be an invertible $n \times n$ matrix.
    For any $b \in \R^n$ the unique solution $x$ of $Ax = b$ has entries
    given by
    $$
        x_i = {\det A_i(b)}{\det A}, \qquad i = 1, \dotsc, n
    $$
    \end{theorem}

    \begin{compactdesc}
    \item[Proof of Th \ref{th-cramer}] If $Ax = b$, then
    $A \cdot I_i(x) =
    \begin{bmatrix} Ae_1 & \cdots & Ax & \cdots & Ae_n \end{bmatrix}
    =
    \begin{bmatrix} a_1 & \cdots & b & \cdots & a_n \end{bmatrix}
    = A_i(b)$.
    By the multiplicative property of determinants,
    $\det A \det I_i(x) = \det A_i(b)$.
    But $\det I_i(x) = x$ because it is a diagonal matrix.
    Because $A$ is invertible, $\det A \neq 0$ and it follows that
    $x = \det A_i(b) / \det A$.
    \item[Adjugate or classical adjoint] of an $n \times n$ matrix $A$ is
        the matrix formed by taking the cofactors of every element in
        $A$ and then transposing the resulting matrix:
        $$
        adj \@ A =
            \begin{bmatrix} C_{11} & \cdots & C_{1n}
                         \\ \vdots & \ddots & \vdots
                         \\ C_{n1} & \cdots & C_{nn}
            \end{bmatrix}^T
        $$
    \end{compactdesc}

    \begin{theorem}[Inverse formula]
        Let $A$ be an invertible $n \times n$ matrix. Then
        $$ A^{-1} = \frac{1}{\det A} adj \@ A $$
    \end{theorem}

    \end{card}
    \begin{card}

    \begin{theorem}[Simple area and volume]\label{th-area-vol}
    If $A$ is a $2 \times 2$ matrix, the area of the parallelogram determined
    by the columns of $A$ is $|\det A|$. The volume of a parallelepiped (3D)
    determined by the columns of $A$ is also $|\det A|$.
    \end{theorem}

    \begin{theorem}[Linear trans. and area or vol.]\label{th-trans-area-vol}
        Let $T: \R^2 \to \R^2$ be a linear transformation determined by a
        matrix $A$. If $S$ is a parallelogram in $\R^2$, then
        $$
            \text{area of } T(S) = |\det A| \cdot \text{area of } S
        $$
        Likewise for a parallelepiped $S$ with everything in $\R^3$:
        $$
            \text{volume of } T(S) = |\det A| \cdot \text{volume of } S
        $$
    \end{theorem}

    \begin{compactdesc}
    \item[Example area of] parallelogram with vertices at points
        $(-2,-2), (0,3), (4,-1), (6,4)$. First translate it to the origin
        $(0,0), (2,5), (6,1), (8,6)$. The area of this figure is
        $$
        \det\begin{bmatrix} 2 & 6 \\ 5 & 1 \end{bmatrix} = 28
        $$

    \item[Proof of Th \ref{th-area-vol}]
        % TODO
    \item[Proof of Th \ref{th-trans-area-vol}]
        % TODO
    \item[Generalization of Th \ref{th-trans-area-vol}] This theorem holds for
        any region with finite area in $\R^3$ or finite volume in $\R^3$.
    \item[Example with ellipse] which has equation $x^2/a^2 + y^2/b^2 \leq 1$.
        It can be transformed into a unit sphere by the transformation
        with standard matrix
        $\left[\begin{smallmatrix}
        a & 0 & \\ 0 & b
        \end{smallmatrix}\right]$ with determinant $ab$.
        Because a unit sphere has volume $\pi$, the ellipse has volume $ab\pi$.
    \item[Circle] Equation of a circle $(x - a)^2 + (y - b)^2 = \sqrt r$
        which crosses three points
        $(x_1, y_1), \dotsc, (x_3, y_3)$ is given by:
        $$
        \det \begin{bmatrix}
        x^2 + y^2 & x & y & 1
     \\ x_1^2 + y_1^2 & x_1 & y_1 & 1
     \\ x_2^2 + y_2^2 & x_2 & y_2 & 1
     \\ x_3^2 + y_3^2 & x_3 & y_3 & 1
        \end{bmatrix}
        = 0
        $$
    \item[Geometric equations] can be expressed in terms of determinants.
        Similar expressions work for general equations of lines of any dimension,
        spheres, cones, etc. \dots
    \end{compactdesc}
\end{card}
\section{Vector Spaces}
\begin{outline}

\end{outline}

\begin{card}
    \subsection{Vector Spaces and Subspaces}

    \begin{theorem}[Vector Space Axioms]
    A nonempty set $V$ with two binary operations called
    addition and multiplication by scalars (in this case, real numbers).
    Axioms must hold for all $u,v,w \in V$ and $c,d \in \R$:
    \begin{compactenum}
    \item $u + v \in V$
    \item $u + v = v + u$
    \item $u + (v + w) = (u + v) + w$
    \item There exists a zero vector such that $u + 0 = u$
    \item There exist inverse vectors $-v$ such that $v + (-v) = 0$
    \item $cu \in V$
    \item $c(u + v) = cu + cv$
    \item $(c + d)u = cu + du$
    \item $c(du) = (cd)u$
    \item There exists a vector such that $1u = u$
    \end{compactenum}
    \end{theorem}
    \end{card}

    \begin{card}
    \begin{compactdesc}
    \item[Many properties] can be shown using the ten axioms
    \item[Negative] shorthand $-u$ = $(-1)u$.
    \item[Examples] polynomials of n-degree $\mathbb{P}_n$, $\R^n$, $\Z_n$, continuous functions
        % signals!!!
    \item[Subspace] of a vector space $V$ is a subset $H$ of $V$ with:
    \begin{compactenum}
    \item $0 \in H$
    \item For all $u, v \in H$, $u + v \in H$
    \item For every scalar $c$ and vector $v \in H$, $cv \in H$
    \end{compactenum}

    \item[Subspaces are also] vector spaces
    \item[Zero subspace] $\{0\}$ a trivial subspace of any vector space
    \item[Example subspaces] polynomials of degree $\leq n$ in vector space $\mathbb{P}_n$,
        polynomials are a subspace of the vector space formed by continuous functions
    \item[Linear combination] of vectors in vector space $V$ is
        $c_1v_1 + c_2v_2 \in V$ for any scalars $c_1, c_2$.
    \item[Span] of a set of $n$ vectors is the set formed by taking all scalars
    $c_1, \dotsc, c_n$ and creating linear combinations of vectors
    Span $\{v_1, \dotsc, v_n \} = c_1 v_1 + \dotsb + c_n v_n$.
    \item[Synonyms] span of vectors is the \textbf{subspace spanned or generated} by
        the vectors. A \textbf{generating set} for a subspace is a set of vectors
        such that they span the subspace.
    \end{compactdesc}

    \begin{theorem}
    If $v_1, \dotsc, v_n$ are in a vector space $V$ then Span $\{v_1, \dotsc, v_n \}$
    forms a subspace of $V$.
    \end{theorem}
\end{card}


\begin{card}
    \subsection{Null Spaces, Column Spaces, Linear Transformations}

    \begin{compactdesc}
    \item[Subspace interpretation] either arise as solutions to a system
        of homogeneous linear equation, or as the set of linear combinations
        of some vectors.
    \item[Null space] of an $m \times n$ matrix $A$ is the set of all
        solutions to $Ax = 0$. Dynamic description: all $x \in \R^n$ mapped
        into $0 \in \R^m$ by the mapping $x \mapsto Ax$.
        \textbf{Implicitly} defined.
        $\text{Nul } A = \{x | x \in \R^n \text{ and } Ax = 0 \}$.
    \end{compactdesc}
    \begin{theorem}%
        [Null space is subspace]\label{th-null-sub}
    The null space of an $m \times n$ matrix is a subspace of $\R^n$.
    \end{theorem}
    \begin{compactdesc}
    \item[Proof of Th. \ref{th-null-sub}]
        % TODO
    \item[Explicit description] of a null space is found by row reducing $A$
        and finding the general solution to $Ax = 0$.
    \item[Solution set] formed by a spanning set is automatically
        linearly inependent because free variables are the weights on the
        vectors.
    \item[Number of free variables] is the number of vectors in the
        null space.
    \item[Column space] of an $m \times n$ matrix $A$ is the set of all
        linear combinations of the columns of $A$.
        $\text{Col } A = Span \{a_1, \dotsc, a_n\}$.
    \item[About null space]:

        \begin{compactenum}
        \item A subspace of $\R^n$
        \item Implicitly defined
        \item Easy to tell if vector is present, check $Ab = 0$
        \item Equal to $\{0\}$ iff $Ax = 0$ has only the trivial solution.
        \end{compactenum}
    \item[About column space]:

        \begin{compactenum}
        \item A subspace of $\R^m$
        \item Explictly defined
        \item Hard to tell if vector is present, must row reduce $[A \quad v]$
        \item Equal to $\R^m$ iff $Ax = b$ has a solution for every $b \in \R^m$.
        \end{compactenum}
    \item[Linear transformation] $T$ from a vector space $V$ to a vector space
        $W$ assigns each vector $x \in V$ to a vector $T(x) \in W$.
        Linear if: $T(u + v) = T(u) + T(v)$ and if $T(cu) = cT(u)$ for all
        vectors $u,v \in V$ and all scalars $c \in \R$.
    \item[Kernel] of a transformation $T$ is set of all vectors in $V$ that
        map to $0 \in W$. Solutions to $T(x) = 0$.
    \item[Range] of a transformation $T$ is set of all vectors in $W$ with form
        $T(x)$ for some $x \in V$.
    \item[Example] differentiation is a linear transformation of
        real-valued continuous functions.
    \end{compactdesc}
\end{card}


\begin{card}
    \subsection{Linearly Independent Sets; Bases}

    \begin{theorem}
    A set of two or more vectors with $v_1 \neq 0$ is
    linearly dependent iff some $v_j$ with $j > 1$ is a linear combination
    of the other vectors.
    \end{theorem}

    \begin{compactdesc}
    \item[Linear independence] Vectors $v$ are linearly independent
        if only $c_i \neq 0$ can satisfy
        the equation: $c_1 v_1 + \dotsb + c_p v_p = 0$.
    \item[Linear dependence relation] formed when $c_i \neq 0$ for some $c_i$.
    \item[Basis] for a subspace $H$ of a vector space $B$ is formed by the
        set $\mathcal{B} = \{b_1, \dotsc, b_p \}$ if
        \begin{compactenum}
        \item $\mathcal{B}$ is a linearly independent set
        \item Span $\mathcal{B} = H$.
        \end{compactenum}
    \item[Standard basis] for $\R^n$ is the set $\{e_1, \dotsc, e_n \}$.
    \item[Standard basis] for $\PP_n$ is the set $\{ 1, t, t^2, \dotsc, t^n \}$.
    \end{compactdesc}

    \begin{theorem}
    Let $S = \{v_1, \dotsc, v_n \}$ be a set in $V$ and let Span $S = H$.
    \begin{compactenum}
    \item If one vector in $S$ is a linear combination of the other vectors
        in $S$, then the set formed by removing this vector still spans $H$.
    \item If $H \neq \{0\}$, some subset of $S$ is a basis for $H$
    \end{compactenum}
    \end{theorem}

    \begin{compactdesc}
    \item[Basis for Null Space] is easy, the null space is its own basis.
    \item[Rowops] Elementary row operations on a matrix do not affect the
        linear dependence relations among the columns of the matrix.
    \item[Proof of Th.~\ref{th-pivot-basis}]
        % TODO
    \item[Two views] a basis can be
    \begin{compactenum}
    \item linearly independent set (as large as possible)
    \item spanning set (as small as possible)
    \end{compactenum}

    \end{compactdesc}
    \begin{theorem}\label{th-pivot-basis}
    The pivot columns of a matrix $A$ form a basis for Col $A$.
    \end{theorem}
\end{card}



\begin{card}
    \subsection{Coordinate Systems}

    \begin{theorem}[Unique Representation Theorem]
    Let $\mathcal{B} = \{b_1, \dotsc, b_n\}$ be a basis for a vector space
    $V$. Then for each $x \in V$, there exist unique scalars
    $[x]_\mathcal{B} = \begin{bmatrix} c_1 \\ \vdots \\ c_n \end{bmatrix}$
    such that
    $$
        x = c_1b_1 + \dotsb + c_n b_n
    $$
    \end{theorem}
    \begin{theorem}[Change of coord is 1-to-1, linear]
    Let $\mathcal{B}$ be a basis for a vector space $V$.
    Then the coordinate mapping $x \mapsto [x]_\mathcal{B}$ is a
    1-to-1 linear transformation from $V$ onto $\R^n$
    \end{theorem}

    \begin{compactdesc}
    \item[Coordinates] of $x$ relative to the basis $\mathcal{B}$ are the
        weights $c_1, \dotsc, c_n$.
    \item[Change of coord.s matrix] $P_\mathcal{B} = [b_1 \cdots b_n]$.
    \item[Change of coordinates] in $\R^n$. If there is another basis for
        $\R^n$, then $x = P_\mathcal{B}[x]_\mathcal{B}$.
    \item[Isomorphism] between $\R^n$ and any $n$ dimensional subspace is
        defined by the transform $x \mapsto [x]_\mathcal{B}$.
    \end{compactdesc}
\end{card}


\begin{card}
    \subsection{Dimensions of a Vector Space}

    \begin{theorem}[Dimension and Dependence]
    If a vector space $V$ has a basis with $n$ vectors, then any set in
    $V$ containing more than $n$ vectors must be linearly dependent.
    \end{theorem}% proof

    \begin{theorem}[Size of Every Basis]% proof
        If a vector space $V$ has a basis of $n$ vectors,
        then every basis of $V$ has $n$ vectors.
    \end{theorem}% proof
    \begin{compactdesc}
    \item[Finite dimensional] vector spaces are spanned by a
        finite number of vectors. If it's not, it is \textbf{infinite dimensional}.
    \item[Zero vector space] $\{0\}$ is defined to be zero
    \item[Example] $\dim = \infty$, the set of all polynomials
    \item[Dimension of null] space is number of free variables
    \item[Dimension of column] space is number of basic variables
    \end{compactdesc}

    \begin{theorem}
    Let $H$ be a subspace of a finite dimensional vector space $V$.
    Any linearly independent set can be expanded (add more linearly independent
    vectors) to form a basis for $H$.

    Also, $H$ is finite dimensional and:
    $$\dim H \leq \dim V$$
    \end{theorem}
    \begin{theorem}[Basis Theorem]
    Let $V$ be a set with $p$ dimensions.
    Any set of $p$ linearly independent vectors forms a basis for $V$.

    Any set of $p$ vectors that span $V$ form a basis for $V$.
    \end{theorem}
\end{card}


\begin{card}
    \subsection{Rank}

    \begin{theorem}\label{th-row-rowequiv}
    Two row equivalent matrices $A$ and $B$ share the same row space.

    If $B$ is in echelon form, the nonzero rows of $B$ form a basis for the
    row space of $A$ and for $B$.
    \end{theorem}

    \begin{compactdesc}
    \item[Row space] set of all linear combinations of the row vectors of a
        matrix
    \item[Th. \ref{th-row-rowequiv}]
    \item[Basis for Col] space of a matrix: pivot columns of the matrix
    \item[Basis for Row] space of a matrix: rows a row equivalent matrix
        in echelon form
    \item[Basis for Nul] space of a matrix: vectors in the vector form of
        the general solution to $Ax = 0$.
    \item[Rank] of $A = \dim$ Col $A$.
    \item[Dimension of Row] space $= \dim$ Col $A = \dim$ Col $A^T$
    \end{compactdesc}
    \begin{theorem}[The Rank Theorem]\label{th-rank}
    The dimensions of the column space and the row space of an $m \times n$
    matrix $A$ are the same. This common dimension is equal to the
    number of pivot positions in $A$ and
    $$ \text{rank } A + \dim \text{Nul } A = n$$
    \end{theorem}
    \begin{compactdesc}
    \item[Proof of Th. \ref{th-rank}]
        % TODO
    \item[Example] if $A$ is a $7 \times 9$ matrix, what's is rank?

        The equation $r + 2 = 9$ must be satisfied, so the rank is $7$.
    \item[Example] Could a $6\times9$ matrix have $\dim$ Nul $A = 2$?

        No. If it did, then the rank should be $7$, but there's only $6$ columns!
    \item[Proof of IMT cont. \ref{th-imt-rank}]
        % TODO
    \end{compactdesc}
    \begin{theorem}[Rank and IMT, cont.]\label{th-imt-rank}
    Let $A$ be an $n\times n$ matrix, these statements are equivalent to the
    statement that $A$ is an invertible matrix:
    \begin{compactenum}
    \item The columns of $A$ form a basis for $\R^n$
    \item Col $A = \R^n$.
    \item $\dim \text{Col } A = n$
    \item rank $A = n$
    \item Nul $A = \{0\}$
    \item $\dim$ Nul $A = 0$
    \end{compactenum}
    \end{theorem}
\end{card}
\section{Eigenvalues and Eigenvectors}
\begin{outline}
\end{outline}

\begin{card}
    \subsection{Definition}

    \begin{compactdesc}
    \item[Eigenvector] of an $n \times n$ matrix $A$ is any nonzero vector $x$
        such that $Ax = \lambda x$ for some scalar called an \textbf{eigenvalue}
        $\lambda$.
    \item[Eigenvalue] $\lambda$ exists if there is a nontrivial solution $x$
        of $Ax = \lambda x$.
        This $x$ is an \textsl{eigenvector corresponding to $\lambda$}.
        This scalar may be $0$.
    \item[Find eigenspace] given eigenvalue $\lambda$: solution set to
        $Ax = \lambda x$ or equivalently $(A - \lambda I) x = 0$.
        The null space of the matrix $A - \lambda I$.
    \item[Difference Equations] can be solved using eigenvalues.
        If $x_{k+1} = Ax_k$ for $k \in \Z_{+,0}$, then an explicit solution to
        the system is $x_k = \lambda^k x_0$ for $k \in \Z_+$.
    \item[Inductive step] this works because:
        $Ax_k = A(\lambda^k x_0) = \lambda^k (A x_0) = \lambda^k(\lambda x_0) = x_{k+1}$.
        Linear combinations of the form $\lambda^k v$ are also solutions.
    \end{compactdesc}

    \begin{theorem}[Eigenvalues of Triangular]
    The eigenvalues of a triangular matrix are the entries on its main diagonal.
    \end{theorem}

    \begin{theorem}[Linear Independence of Eigenvectors]
    If $v_1, \dotsc, v_r$ are eigenvectors corresponding to distinct eigenvalues
    $\lambda_1, \dotsc, \lambda_r$ of a matrix,
    then the set $\{v_1, \dotsc, v_r\}$ is linearly independent.
    \end{theorem}

\end{card}


\begin{card}
    \subsection{Characteristic Equation}

    \begin{compactdesc}
    \item[Motivation] the determinant of a matrix is zero iff it is not
        invertible; if this is the case then its Null space has non-zero
        dimension.
    \item[Determinants] see chapter 3
    \item[Characteristic Eqn.] a scalar $\lambda$ is an eigenvalue of an
        $n \times n$ matrix $A$ iff $\lambda$ satisfies the characteristic
        equation: $\det A - \lambda I = 0$. Also called characteristic
        polynomial of degree $n$.
    \item[Multiplicity] of an eigenvalue is its multiplicity in the
        characteristic equation. Example: the characteristic polynomial
        $(\lambda - 3)^5 = 0$ reveals an eigenvalue $3$ with multiplicity $5$.
    \item[Similarity] $n \times n$ matrices $A$ and $B$ are similar if
        there exists an invertible matrix $P$ such that $A = PBP^{-1}$.
    \item[Similarity Transformation] the act of changing $A$ into $P^{-1}AP$.
    \item[Application to Dynamical] Systems. Let $A$ be an $n \times n$ matrix.
        Let $x_0$ be the initial state with
        coordinates $c_1, \dotsc, c_n$ with respect
        to the basis formed by eigenvectors $v_1, \dotsc, v_n$.

        If all the eigenvalues are in $[-1,1]$, then the system converges to a
        steady state as $k \to \infty$.
        It could also diverge, or just converge to zero.

        The system $x_{k+1} = Ax_k$ has an explicit solution
        $x_k = c_1 \lambda_1^k v_{\lambda 1} + \dotsb + c_n \lambda_n^k v_{\lambda n}$.
    \end{compactdesc}

    \begin{theorem}[Invertible Matrix Th. (cont.)]
    Let $a$ be an $n \times n$ matrix. Then $A$ is invertible iff
    \begin{compactenum}
    \item The number $0$ is not an eigenvalue of $A$
    \item $\det A \neq 0$
    \end{compactenum}
    \end{theorem}

    \begin{theorem}[Properties of Determinants]
    --- see section \ref{ch-dets} ---
    \end{theorem}

    \begin{theorem}[Similarity and Eigenvalues]
    If $n \times n$ matrices $A$ and $B$ are similar, then they have the same
    characteristic polynomial and the same eigenvalues with the same
    multiplicities.
    \end{theorem}

    WARNING: similarity is not the same as row equivalence. Row operations on
    a matrix usually change its Eigenvalues.

\end{card}


\begin{card}
    \subsection{Diagonalization}

    \begin{compactdesc}
    \item[Diagonalizable] square matrix is similar to a very specific
        diagonal matrix.
    \item[Eigenvector Basis] is formed by the set of all eigenvectors for
        all eigenvalues of a diagonalizable $n \times n$ matrix.
        Because it contains enough independent vectors,
        this set forms a basis for $\R^n$.
    \item[Diagonalizing Matrices] To factor an $n \times n$ matrix $A$ into
        form $A = PBP^{-1}$, four steps are required.
    \begin{compactenum}
    \item Find eigenvalues of $A$. Use computer or characteristic equation.
    \item Find $n$ linearly independent vectors of $A$.
        Procedure: find the null space of matrix $A - \lambda I$.
        \textbf{Check for linear independence} amongst the vectors.
    \item Construct $P$ from the eigenvectors. Order unimportant.
        \textbf{Check $P$ is invertible}.
    \item Construct $D$ from the corresponding eigenvalues.
        \textbf{Same order} as $P$.
    \end{compactenum}
    \end{compactdesc}


    \begin{theorem}[Diagonalization Theorem]
    If an $n \times n$ matrix $A$ has $n$ distinct eigenvalues,
    then it is diagonalizable.
    \end{theorem}

\end{card}
\begin{card}
    \begin{compactdesc}
    \item[Powers of a matrix] with form $A = PDP^{-1}$ can be easily calculated
        $A^k = PD^kP^{-1}$.
    \item[Functions on a matrix] with form $A = PDP^{-1}$ can also be easily
        calculated. For example, to take the sine simply take the sine of the
        diagonal entries of $D$: $\sin A = P (\sin D) P^{-1}$.
        Found by using power series of $\sin, \exp$, etc.
    \item[Non-distinct Eigenvalues]
    If the matrix $A$ has less than $n$ eigenvalues, it is still possible
    for $A$ to be diagonalizable. See Th.~\ref{th-nn-n-distinct}.
    \end{compactdesc}

    \begin{theorem}[$n \times n$ and $p$ distinct eigenvalues]\label{th-nn-n-distinct}
    Let $A$ be an $n \times n$ matrix
    whose distinct eigenvalues are $\lambda_1, \dotsc, \lambda_p$.
    \begin{compactenum}
    \item For $1 \leq k \leq p$, the dimension of the eigenspace for $\lambda_k$
        is less than or equal to the multiplicity of the eigenvalue $\lambda_k$.
    \item The matrix $A$ is diagonalizable iff the sum of the dimensions of the
        distinct eigenspaces equals $n$.

        This only happens when the dimensions of the eigenspace for each
        eigenvalue $\lambda_k$ equals the multiplicity of $\lambda_k$.
    \item If $A$ is diagonalizable and $\B_k$ is a basis for the eigenspace
        corresponding to $\lambda_k$, then the total collection of vectors
        in the sets $\B_1, \dotsc, \B_p$
        forms an \textbf{eigenvector basis} for $\R^n$.
    \end{compactenum}
    \end{theorem}

\end{card}


\begin{card}
    \subsection{Eigenvectors and Linear Transformations}

    \begin{compactdesc}
    \item[Linear Trans. and $D$] a linear transformation $T(x) = Ax$
        with a diagonalizable standard matrix $A$
        can be represented using a simpler transformation of the form:
        $u \mapsto Du$.
    \item[Linear Transformation] Let $V, W$ be vector spaces where
        $\dim V = n$ and $\dim W = m$. Let $T: V \to W$. Choose two ordered
        bases $\B = \{b_1, \dotsc, b_n\}, \mathcal{C}$ for $V,W$ respectively.

    \item[Matrix-based] representation for the linear transformation $T$ is
        $T(v) = [M[x]_\B]_\mathcal{C}^{-1}$. As far as coordinate vectors go,
        $T$ is just a left-multiplication by $M$.
    \item[Matrix Relative to Bases] $\B$ and $\mathcal{C}$ is the $m \times n$
        matrix:
        $M = \begin{bmatrix}
        [T(b_1)]_\mathcal{C} & \cdots & [T(b_n)]_\mathcal{C}
        \end{bmatrix}$.
    \item[Change-of-coordinates] matrix is created when $V=W$ and
        $T$ is the identity function.
    \item[Example] suppose $\B = \{b_1, b_2\}$ is a basis for $V$ and
        $\mathcal{C} = \{c_1, c_2, c_3\}$ is a basis for $W$. Let
        $T : V \to W$ be a linear transformation with the property that
        $T(b_1) = 3c_1 - 2c_2 + 5c_3$ and $T(b_2) = 4c_1 + 7c_2 - c_3$.

        The matrix $M$ relative to $\B$ and $\mathcal{C}$ is
        $$
        M = [[T(b_1)]_\mathcal{C} \quad [T(b_2)]_\mathcal{C}]
        = \begin{bmatrix} 3 & 4 \\ -2 & 7 \\ 5 & -1 \end{bmatrix}
        $$
\end{compactdesc}
\end{card}

\begin{card}
\begin{compactdesc}
    \item[From V into V] (linear transformations)
        If $T: V \to V$, then the matrix $M$ is called the matrix for $T$
        relative to $\B$ or $\B$-matrix for $T$ denoted by $[T]_\B$.
        Then $[T(x)]_\B = [T]_\B [x]_\B$.
    \item[Example] mapping $T: \PP_2 \to \PP_2$ is defined by
        $T(a_0 + a_1t + a_2t^2) = a_1 + 2a_2t$.
        The basis $\B = \{1,t,t^2\}$.

        The $\B$-matrix for $T$ is $T(1) = 0; T(t) = 1; T(t^2) = 2t$.
        The coordinates of these polynomials are used (in order) as columns:
        $$
        [T]_\B = \begin{bmatrix}0 & 1 & 0 \\ 0 & 0 & 2 \\ 0 & 0 & 0 \end{bmatrix}
        $$
        For a general polynomial $T(a_0 + a_1t + a_2t^2)
        = [T]_\B \begin{bmatrix}a_0 \\ a_1 \\ a_2 \end{bmatrix}$.
    \item[Linear Trans. on $\mathbb{R}^n$] if the transformation is $T(x) = Ax$
        and $A$ is diagonalizable, then the $\B$-matrix for $T$ is
        a diagonal matrix.
    \item[Similarity] of Matrix Representations
    \item[Efficient computation] of a $\B$-matrix $P^{-1}AP$ ...
    \end{compactdesc}

    \begin{theorem}[Diagonal Matrix Representation]
    Suppose $A = PDP^{-1}$ where $D$ is an $n \times n$ diagonal matrix.
    If $\B$ is the basis of $\R^n$ formed from the columns of $P$,
    then $D$ is the $\B$-matrix representation for the transformation
    $x \mapsto Ax$.

    That is to say, the mappings $x \mapsto Ax$ and $u \mapsto Du$ are the
    same linear transformation relative to different bases.
    \end{theorem}

\end{card}


\begin{card}
    \subsection{Complex Eigenvalues}

    \begin{compactdesc}
    \item[Complex eigenvalues] allowed? Then $A$ must act on vectors in $\C^n$.
        Linear algebra can be rebuilt using $C^n$ instead of $\R^n$,
        but that is not our concern here.
    \item[Finding complex eigenvalues], these arise naturally from the Fundamental
        Theorem of Algebra.
    \item[Finding complex eigenvectors] use a machine for row reducing complex
        matrices.
    \item[Rotations] a real matrix with complex eigenvalues always defines
        a rotation of vectors.
    \item[Complex conjugate] of a complex number $\overline{a + bi} = a - bi$.
        For a matrix, just take the complex conjugate of each entry.
        Properties for complex numbers, vectors and matrices:
        $$\overline{rx} = \overline{r}\overline{x}
        , \quad \overline{Bx} = \overline{B}\overline{x}
        , \quad \overline{BC} = \overline{B}\overline{C}
        , \quad \overline{rB} = \overline{r}\overline{B}$$
    \item[Complex to real] Let $c \in \C$. Then $\overline{c} \in C$
        but $c\overline{c} \in \R$. This is why a real matrix' eigenvalues
        come in conjugate pairs.
    \item[Conjugate pairs] if a real matrix $A$ has complex eigenvalue $a + bi$
        then $a - bi$ is also an eigenvalue.
    \item[Scale and rotate] consider the matrix
        $C = \left[\begin{smallmatrix}a & -b \\ b & a\end{smallmatrix}\right]$.
        Its eigenvalues are $\lambda_{1,2} = a \pm bi$. Let $r = |\lambda_1|$
        and $\phi$ be the angle between ray from $0$ to $(a,b)$.
        Then the matrix represents a scaling followed by a rotation transformation:
        $C = \left[\begin{smallmatrix} r & 0 \\ 0 & r \end{smallmatrix}\right]
        \left[\begin{smallmatrix} \cos\phi & -\sin\phi \\ \sin\phi & \cos\phi \end{smallmatrix}\right]$.
    \item[Rotation in any matrix] with complex eigenvalues $A$.
        Any matrix with complex eigenvalues is similar to $C$.
        Therefore, multiplication by $A$ represents a change of coordinates
        wrapping a rotation $PCP^{-1}$.
        occur in complex conjugate pairs.
        Then $A^kx$ can trace out an ellipse instead of a circle.
    \end{compactdesc}
    \begin{theorem}[Complex Eigenvalues and Rotations]
    Let $A$ be a real $2\times2$ matrix with a complex eigenvalue pair $a \pm bi$.
    where $b \neq 0$ and an associated vector $V \in \C^2$.
    Then $A = PCP^{-1}$ where $P = [\text{Re } v \quad \text{Im } v]$
    and $C = \left[\begin{smallmatrix}a & -b \\ b & a\end{smallmatrix}\right]$.
    \end{theorem}
\end{card}


\begin{card}
    \subsection{Discrete Dynamical Systems}

    \begin{compactdesc}
    \item[Eigenvector decomposition] of a discrete dynamical system
        $x_{k+1} = Ax_k$ determines the limit of the sequence $\{x_k\}$.
        If $A$ has $m$ eigenvalues $\lambda_i$ and $x_0$ has coordinates
        $c_i$ with respect to eigenvector basis $v_i$ formed by the eigenvalues then
        $x_k = c_1\lambda_1^kv_1 + \dotsb + c_m\lambda_m^kv_m$.
    \item[Convergence/divergence] the system converges if every eigenvalue
        has magnitude less than or equal to $1$.
        The system diverges if at least one eigenvalue is greater than $1$
        in magnitude.
    \item[Trajectory] graphing the sequence $x_0, x_1, \dotsc$.
    \item[2D Systems] have several types of trajectories.
    \item[Boring] solutions are simple when $c = 0$ or when the eigenvalues are $1$.
    \item[Attractor] trajectories converge to the origin if both eigenvalues
        are real and $< 1$ in magnitude
    \item[Repellor] trajectories diverge if both eigenvalues
        are real and $> 1$ in magnitude
    \item[Saddle point] trajectories converge to a line but diverge away from the
        origin if both eigenvalues are real and
        one is $<1$ and the other is $>1$ in magnitude.
    \item[Nondiagonal case] %TODO
    \item[Complex eigenvalues] are conjugate pairs, so they have the same
        magnitude.
    \item[Spiral outward] trajectories diverge and rotate
        if complex eigenvalues have magnitude $>1$.
    \item[Spiral inward] trajectories converge and rotate
        if complex eigenvalues have magnitude $<1$.
    \item[Elliptical trajectory] trajectories are rotated
        if complex eigenvalues have magnitude $1$.
    \end{compactdesc}

\end{card}


\begin{card}
    \subsection{Applications to Differential Equations}

    \begin{compactdesc}
    \item[Differential Equation]
    \item[Fundamental set of solutions]
    \item[Initial value problem]
    \item[Eigenfunctions]
    \item[Trajectory]
    \item[Attractor/sink]
    \item[Repellor/source]
    \item[Saddle point]
    \item[Complex eigenvalues]
    \item[Spiral point]
    \end{compactdesc}

\end{card}


\begin{card}
    \subsection{Iterative Estimates for Eigenvalues}

    \begin{compactdesc}
    \item[Strictly Dominant Eigenvalue]
    \item[The power method] can approximate the strictly dominant eigenvalue of
        a matrix:
        \begin{compactenum}
        \item
        \item
        \item
        \end{compactenum}
    \item[Inverse power method] provides approximation for any eigenvalue:
        \begin{compactenum}
        \item
        \item
        \item
        \item
        \end{compactenum}
    \item[]
    \item[]
    \item[]
    \item[]
    \end{compactdesc}

\end{card}


\section{Orthogonality and Least Squares}
\begin{outline}
\end{outline}

\begin{card}
    \subsection{Inner Product, Length, Orthogonality}

    \begin{compactdesc}
    \item[Inner Product] $u, v$ are $n \times 1$ matrices, their
        inner product is $u^Tv$, a scalar.
        AKA \textbf{dot Product}.
    \item[Length or norm of a vector] defined by $\norm{v} = \sqrt{v \cdot v}$.
        Property: $\norm{cv} = c\norm{v}$.
    \item[Unit Vector] has norm $1$. \textbf{Normalizing} a vector is done by
        dividing by its norm $u = v\norm{v}$, then $\norm{u} = 1$.
    \item[Distance] between vectors $u$ and $v$ is given by $\norm{u - v}$.
    \item[Orthogonal vectors] two vectors are orthogonal, or perpendicular if
        $u \cdot v = 0$. This is equivalent to having a $90^\circ$ angle
        between them.
    \item[Vector $\bot$ subspace] if the vector is perpendicular to every
        vector in the subspace. Equivalently, it is perpendicular to every basis
        vector of the subspace.
    \item[Orthogonal complement] let $W$ be a subspace of $\R^n$.
        The orthogonal complement of $W$ is $W^\bot$ pronounced ``W perp.''
        It is defined as the set of all vectors perpendicular to the vectors in
        $W$, $W^\bot = \{v \bot W | v \in \R^n \}$.
    \item[New Subspace]
        If $W$ is a subspace of $\R^n$, then $W^\bot$ is a subspace of $\R^n$.
    \item[Angle] between two vectors $u$ and $v$ is $\theta$ and found using:
        $\cos \theta = u \cdot v / \norm{u}\norm{v}$.
        This formula works for higher dimensions, in statistics $\cos\theta$
        is called the correlation coefficient between two vectors.
    \end{compactdesc}

    \begin{theorem}[Inner Product Algebra]
    Let $u,v,w \in \R^n$ and let $c \in \R$. Then
    \begin{compactenum}
    \item $u \cdot v = v \cdot u$
    \item $(u + v) \cdot w = u \cdot w + v \cdot w$
    \item $(cu)\cdot v = u \cdot (cv) = c(u \cdot v)$
    \item $u \cdot u \geq 0$ and $u \cdot u = 0$ iff $u = 0$
    \end{compactenum}
    \end{theorem}

    \begin{theorem}[Pythagorean Theorem]
    Two vectors $u$ and $v$ are orthogonal iff
    $\norm{u + v}^2 = \norm{u}^2 + \norm{v}^2$.
    \end{theorem}

    \begin{theorem}[Orthog. Complement of a Row Space]
    Let $A$ be an $m \times n$ matrix. Then
        $$ (\text{Col } A)^\bot = \text{Nul } A^T
    \qquad (\text{Col } A^T)^\bot = \text{Nul } A $$
    \end{theorem}


\end{card}


\begin{card}
    \subsection{Orthogonal Sets}

    \begin{theorem}[Orthog. Set, Linearly Independent]
    If $S$ is an orthogonal set of nonzero vectors in $\R^n$ then $S$
    is linearly independent and a basis for the subspace spanned by $S$.
    \end{theorem}

    \begin{compactdesc}
    \item[Orthogonal Set] $\{u_1, \dotsc, u_p\}$
        is formed if each element is perpendicular to every other element.
        That is, $u_i \cdot u_j = 0$ if $i \neq j$.
    \item[Orthogonal Basis] A basis for a subspace of $\R^n$ that is also
        an orthogonal set.
    \item[Orthogonal Projection] given a vector $u$ it is possible to
        decompose a vector $y$ into two components:
        $y = \^y + z$ where $\^y = \alpha u$ and $z \bot u$.
        The vector $\^y$ is found in terms of $u$, $\^y = \dfrac{y\cdot u}{u \cdot u}u$.
        The vector $z$ is easily found, it may be the zero vector.
    \item[Geometric Interpretation of Th \ref{th-ortho-basis}]
        Theorem $\ref{th-ortho-basis}$ decomposes a vector into a sum of
        orthogonal projections onto one-dimensional subspaces (lines).
        Like graph paper with a rectangular grid.
    \item[Orthonormal Sets] an orthogonal set composed of unit vectors.
        Very important.
    \item[Orthonormal basis] an orthogonal set forms such a basis for the
        subspace spanned by the set.
    \item[Orthogonal Matrix] a matrix with columns that form an orthonormal set.
        Notice not an orthogonal set (not as useful).
    \item[Rows of an orthogonal] matrix also form an orthonormal set.
    \item[Meaning of Th \ref{th-ortho-algebra}] multiplication by the matrix
        $U$ preserves length and angle of vectors.
    \end{compactdesc}

    \begin{theorem}[Orthog. Basis and Linear Combo.]\label{th-ortho-basis}
    Let $\{u_1, \dotsc, u_p\}$ be an orthogonal basis for a subspace $W$ of
    $\R^n$. Then the weights for each $y \in W$ in the linear combination
    $y = c_1u_1 + \dotsb + c_pu_p $
    are given by
    $$c_j = \frac{y \cdot u_j}{u_j \cdot u_j}$$
    \end{theorem}

    \begin{theorem}[Orthogonal Columns]
    An $m \times n$ matrix $U$ has orthonormal columns iff $U^TU = I$.
    \end{theorem}

    \begin{theorem}[Algebra of Orthonormal Columns]\label{th-ortho-algebra}
    Let $U$ be an $m \times n$ matrix with orthonormal columns and let $x, y \in \R^n$.
    Then
    \begin{compactenum}
    \item $\norm{Ux} = \norm{x}$
    \item $(Ux)\cdot(Uy) = x\cdot y$
    \item $(Ux) \cdot (Uy) = 0$ iff $x \cdot y = 0$
    \end{compactenum}
    \end{theorem}
\end{card}



\begin{card}
    \subsection{Orthogonal Projections}

    \begin{theorem}[Orthogonal Decomposition Theorem]\label{th-ortho-decomp}
    Let $W$ be a subspace of $\R^n$ with an orthogonal basis
    $\{u_1, \dotsc, u_p\}$.
    Then each $y \in \R^n$ can be written uniquely as
    $$y = \^y + z$$
    where $\^y \in W$, $z \in W^\bot$, $z = y - \^y$, and
    $$
        \^y = \frac{y^Tu_1}{u_1^Tu_1}u_1 + \dotsb + \frac{y^Tu_p}{u_p^Tu_p}u_p
    $$
    \end{theorem}

    \begin{compactdesc}
    \item[Vector decomposition] a vector can be decomposed into a sum of
        vectors, one in $W$ and one in $W^\bot$.
    \item[Orthogonal Projection] of $y$ onto $W$: the vector $\^y$ often
        written as $\text{proj}_W y$.
    \item[Geometric Interpretation of] Orthogonal Projections: an orthogonal
        projection of a vector is the sum of its projectins onto
        mutually orthogonal one-dimensional subspaces.
    \item[Identity] If $y \in W$
        then $y = \text{proj}_W y$.
    \item[Orthogonal matrix] has \textbf{orthonormal} columns
    \end{compactdesc}

    \begin{theorem}[The Best Approximation]
    Let $W$ be a subspace of $\R^n$, $y \in \R^n$ and $\^y = \text{proj}_W y$.
    Then $\^y$ is the closest point in $W$ to $y$; in the sense that for
    all $v\in \R^n$, $v \neq y$:
    $$ \norm{y - \^y} < \norm{y - v}$$
    \end{theorem}

    \begin{theorem}[Orthonormal Basis and Projections]
    If $\{u_1, \dotsc, u_p\}$ is an orthonormal basis for a subspace $W$ of $\R^n$,
    then
    $$\text{proj}_W y = (y^Tu_1)u_1 + \dotsb + (y^Tu_p)u_p$$
    If $U = [u_1 \quad \cdots \quad u_p ]$, then for all $y \in \R^n$:
    $$\text{proj}_W y = UU^T y$$
    \end{theorem}

\end{card}


\begin{card}
    \subsection{Gram-Schmidt Process}

    \begin{theorem}[The Gram-Schmidt Process]
    Let $W$ be a subspace of $\R^n$ and assume it has a basis $\{x_1, \dotsc, x_p\}$.
    Let
    \begin{align*}
    v_1 &= x_1
    \\
    v_2 &= x_2 -  \frac{x_2^Tv_1}{v_1^Tv_1}v_1
    \\
    \vdots&
    \\
    v_p &= x_p - \frac{x_p^Tv_1}{v_1^Tv_1}v_1 - \dotsb
              - \frac{x_p^Tv_{p-1}}{v_{p-1}^Tv_{p-1}}v_{p-1}
    \end{align*}
    The set $\{v_1, \dotsc, v_p\}$ is an orthogonal basis for $W$
    and $\text{Span } \{v_1, \dotsc, v_p\} = \text{Span } \{x_1, \dotsc, x_p\} = W$.

    \end{theorem}

    \begin{compactdesc}
    \item[A Simple Algorithm] for producing an orthogonal basis for
        a nonzero subspace of $\R^n$.
        Take two linearly independent vectors, subtract the projection
        $z = v_2 - \text{proj}_{v_1}v_2$
        to get another linearly independent but now orthogonal vector.
    \item[Orthonormal Bases] an orthogonal basis composed of normalized vectors.
    \item[QR Factorization of Matrices] finding an orthogonal basis for an
        $m \times n$ matrix using the Gram-Schmidt process is similar to
        factoring $A = QR$ where $Q$ is orthogonal and $R$ is square and upper triangular.
    \item[Finding $R$] because $Q$ is orthogonal $R = Q^{-1}A = Q^TA$.
    \end{compactdesc}

    \begin{theorem}[QR Factorization]
    If $A$ is an $m \times n$ matrix with linearly independent columns, then
    $A$ can be factored as $A = QR$ where $Q$ is an $m \times n$ matrix
    whose columns form an orthonormal basis for Col $A$
    and $R$ is an $n \times n$ upper triangular invertible matrix
    with positive entries on its diagonal.
    \end{theorem}

\end{card}


\begin{card}
    \subsection{Least-Squares Problems}

    \begin{compactdesc}
    \item[Inconsistent systems] arise often in applications. An
        approximate solution is often demanded.
    \item[Least-squares solution] of $Ax = b$ is an $\^x \in \R^n$ such that
        $\norm{b - A\^x} \leq \norm{b - Ax}$ for all $x \in \R^n$.
        Note $b \in \R^m$ and $A$ is an $m \times n$ matrix.
    \item[Least squares error] $\norm{b - \^b}$
    \item[Soln. of General Least-Squares] Problem.
        Let $Ax = b$ be an inconsistent system.
        Let $\^b = \text{proj}_{\text{Col }A} b$.
        Because $\^b$ is in the column space of $A$, there exist a solution
        $\^x$ to the equation $A\^x = \^b$.
    \item[Optimal] This point $\^b$ is the closest point possible to $b$.
    \item[Proof of Th. \ref{th-least-normal}] By Th~\ref{th-ortho-decomp}
        $b - A\^x$ is in $(\text{Col } A)^\bot$.
        Therefore $a_j^T(b - A\^x) = 0$ for every column $a_j$ of $A$.
        Since $a_j^T$ is a column of $A^T$,
        $A^Tb - A^TA\^x = 0$ and
        $A^Tb = A^TA\^x$. $\square$

        Conversely, suppose $\^x$ satisfies $A^TA\^x = A^tb$.
        % TODO finish
    \item[Normal Equations] of the system $Ax = b$ is this: $A^TAx = A^Tb$.
    \item[QR Factorization] of $A$ can give more reliable results when using
        a machine.
    \end{compactdesc}

    \begin{theorem}[Least-Squares and Normal Equations]\label{th-least-normal}
    The set of least-squares solutions of $Ax = b$ coincides with the nonempty
    set of solutions of the normal equations $A^TAx = A^Tb$.
    \end{theorem}

    \begin{theorem}[Only 1 Least-Squares Soln.]
    The matrix $A^TA$ is invertible iff the columns of $A$ are linearly independent.
    In this case, there exists only one least squares solution $\^x$:
    $$\^x = (A^TA)^{-1} A^T b$$
    \end{theorem}

    \begin{theorem}[Factorization and Linear Indep.]
    Given an $m \times n$ matrix $A$ with linearly idnependent columns, let
    $A = QR$ be a $QR$ factorization of $A$,
    Then for each $b \in \R^m$, the unique least-squares solution to
    $Ax = b$ is
    $$\^x = R^{-1}Q^Tb$$
    \end{theorem}

\end{card}


\begin{card}
    \subsection{Applications to Linear Models}

    \begin{compactdesc}
    \item[Statistical analysis] can be made using the least-squares
        solution of a given system. This is helpful in finding a function
        that fits a set of data.
    \item[Names] Design matrix $X$, parameter vector $\beta$, observation vector $y$.
    \item[Least-Squares Lines] the line $y = \beta_0 + \beta_1 x$ that
        minimizes the sum of the squares of the residuals (offsets from the line).
        The closest fitting line to a set of points.
    \item[Regression Coefficients] found by finding the least-squares
        solution $\beta$ of the system $X\beta = y$.
    \item[Example] find equation that best fits the points
        $(2, 1), (5, 2), (7, 3), (8, 3)$.
        $$X = \begin{bmatrix}1&2\\1&5\\1&7\\1&8\end{bmatrix}
        \qquad
        y = \begin{bmatrix}1\\2\\3\\3\end{bmatrix}$$
        The normal equations $X^TX\beta = X^Ty$ have solution
        $\beta_0 = 2/7$ and $\beta_1 = 5/14$.
        Therefore the least-squares line fitting the data is
        $y = 2/7 + 5x/14$.

    \item[Mean-deviation form] compute average $\overline{x}$ of the original x-values
        and form a new variable $x^* = x - \overline{x}$.
        Then the $X$ matrix is orthogonal and its $QR$ factorization can be
        used for a simpler computation of the solution.
    \item[Residual Vector] $\epsilon = y - X\beta$.
    \item[General Linear Model] any equation of the form $y = X\beta + \epsilon$.
        The matrix $X$ need not model a linear equation, it could be an
        entirely different curve for the data.
    \item[Fitting of Other Curves] if it is best to fit the data to a
        general function $y = \beta_0f_0(x) + \dotsb + \beta_kf_k(x)$
        then the design matrix $X$ is simply changed to
        $$X = \begin{bmatrix}f_0(x_1) & \cdots & f_k(x_k)
                          \\ \vdots & \ddots & \vdots
                          \\ f_0(x_n) & \cdots & f_k(x_n) \end{bmatrix}$$

    \item[Multiple Regression] if an experiment involves two independent
        variables $u,v$ and one dependent variable, we can find a \textbf{trend surface}.
        Data: $(u_0,v_0,y_0), \dotsc, (u_n, v_n, y_n)$.
        Function: $y = \beta_0 f_0(u,v) + \dotsb + \beta_k f_k(u,v)$.
        The design matrix is computed as above.
    \end{compactdesc}
\end{card}


\begin{card}
    \subsection{Inner Product Spaces}

    \begin{compactdesc}
    \item[Inner Product on a Vector Space] $V$ is a function
        $\inner{\cdot,\cdot}: V \times V \to \R$.
        It must satisfy the following axioms for all $u,v,w \in V$ and
        all $c \in \R$:
        \begin{compactenum}
        \item $\inner{u,v} = \inner{v,u}$
        \item $\inner{u+v, w} = \inner{u,w} + \inner{v,w}$
        \item $\inner{cu, v} = c\inner{u,v}$
        \item $\inner{u, u} \geq 0$ and $\inner{u,u} = 0$ iff $u = 0$
        \end{compactenum}
    \item[Inner Product Space] is a vector space with an inner product
    \item[Length] $\norm{v} = \sqrt \inner{v,v}$ for all $v \in V$.
    \item[Distance] the distance between $v,u \in V$ is $\norm{u,v}$.
    \item[Orthogonality] two vectors are orthogonal if $\inner{u,v} = 0$.
    \item[Projection] of a vector onto a subspace with an orthogonal basis
        can be constructed as usual. Has the properties described in
        the Orthognal Decomposition Theorem
        and the Best Approximation Theorem. %TODO ref plz
    \item[Gram-Schmidt Process] works as usual. Can be proven.
    \item[Best Approximation] in Inner Product Spaces.

    \item[Inner Product] for $C[a,b]$
    \end{compactdesc}

    \begin{theorem}[Cauchy-Schwarz Inequality]
    For all $u,v \in V$: $$ |\inner{u,v}| \leq \norm{u}\norm{v} $$
    \end{theorem}

    \begin{theorem}[Triangle Inequality]
    For all $u,v \in V$: $$ \norm{u + v} \leq \norm{u} + \norm{v} $$
    \end{theorem}
\end{card}


\begin{card}
    \subsection{Applications of Inner Product Spaces}

    \begin{compactdesc}
    \item[Weighted least-squares]
    \item[Trend Analysis of Data]
    \item[Fourier Series]
    \end{compactdesc}
\end{card}
\documentclass[letterpaper,9pt]{article}
\usepackage[activate={true,nocompatibility},final,tracking=true,kerning=true,spacing=true]%
           {microtype}
\usepackage[bookmarks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}


% sans serif
\renewcommand*{\familydefault}{\sfdefault}

\usepackage{amsmath, mathtools, amssymb}

% compact lists (compactitem, compactenum, compactdesc)
\usepackage{paralist}

% nicer tables
\usepackage{booktabs}

% sans serif math
% breaks with itemize....
%\usepackage{arevtext,arevmath}

% text columns
\usepackage{multicol}

% frames around text
\usepackage{mdframed}

% section style
\usepackage{sectsty}

% margins
\usepackage[top=0.25in, bottom=0.25in, left=0.2in, right=0.2in]{geometry}

% per-section toc
\usepackage[insection]{minitoc}

\usepackage{thmtools}

\sectionfont{\rmfamily}
\subsectionfont{\rmfamily\centering}

\newenvironment{outline}[1][2]% % number of columns defaults to 2
{\bgroup \footnotesize \begin{multicols}{#1}}%
{\end{multicols} \egroup}

\newenvironment{card}[1][2]% % number of columns defaults to 2
{\bgroup \begin{mdframed} \begin{multicols}{#1}}%
{\end{multicols} \end{mdframed} \egroup}

\newcommand{\bm}[1]{\mathbf{#1}}

\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\PP}{\mathbb{P}}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\inner}{\langle}{\rangle}

\newtheorem{theorem}{Th.}[section]

\begin{document}

\begin{center} {\Large
    {\bf Linear Algebra}
\\  Hugo Rivera}
\\  Math 254, NMT. Summer 2015
\end{center}

\dosecttoc
\setlength{\mtcindent}{5pt}
\begin{multicols}{2}

\tableofcontents
\listoftheorems[ignoreall,show={theorem}]
\end{multicols}

\input{ch1.tex}
\input{ch2.tex}
\input{ch3.tex}
\input{ch4.tex}
\input{ch5.tex}
\input{ch6.tex}

\end{document}
